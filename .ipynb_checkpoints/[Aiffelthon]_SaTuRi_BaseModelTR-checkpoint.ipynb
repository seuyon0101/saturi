{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1220fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "# from tqdm import tqdm_notebook \n",
    "from tqdm.notebook import tqdm \n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75431a56",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e99e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "data_dir = os.getenv('HOME')+'/aiffel/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# test data loading\n",
    "kor_path_test =data_dir+\"/korean-english-park.test.ko\"\n",
    "eng_path_test =data_dir+\"/korean-english-park.test.en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59cad22",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8f1cf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "\n",
    "    assert len(kor) == len(eng) # kor, eng가 같은 갯수라는 것을 검증받기 위해 적용\n",
    "\n",
    "    cleaned_corpus = list(set(zip(eng, kor)))  # 중복된 데이터 제거\n",
    "    \n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(eng_path, kor_path)\n",
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580afea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 데이터 정제 및 토큰화\n",
    "def clean_corpus_test(kor_path_test, eng_path_test):\n",
    "    with open(eng_path_test, \"r\") as f: eng_test = f.read().splitlines()\n",
    "    with open(kor_path_test, \"r\") as f: kor_test = f.read().splitlines()\n",
    "\n",
    "    assert len(kor_test) == len(eng_test) # kor, eng가 같은 갯수라는 것을 검증받기 위해 적용\n",
    "\n",
    "    cleaned_corpus_test = list(set(zip(eng_test, kor_test)))  # 중복된 데이터 제거\n",
    "    \n",
    "    return cleaned_corpus_test\n",
    "\n",
    "cleaned_corpus_test = clean_corpus_test(eng_path_test, kor_path_test)\n",
    "len(cleaned_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab412cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('바트 고든 민주당 하원의원도 “그것은 옳지 못한 행동”이라고 지적했다.',\n",
       "  '\"That\\'s not the \\'right stuff\\' as far as I\\'m concerned,\" said Bart Gordon, D-Tennessee.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabc088d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('올메리트 총리는 “팔레스타인 자치정부와 이스라엘이 이전에 한 번도 이루지 못한 중대한 문제들의 최종 합의에 착수하게 될 것”이라고 덧붙였다.',\n",
       "  '¡°I think we are coming to a moment at which the Palestinian Authority and the state of Israel¡± will undertake serious issues ¡°that will take us finally to a point at which we have never been before ,¡± Olmert added.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5c99b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_corpus+=cleaned_corpus_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc837685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dc3b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모든 입력을 소문자로 변환합니다.\n",
    "# 2. 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
    "# 3. 문장부호 양옆에 공백을 추가합니다.\n",
    "# 4. 문장 앞뒤의 불필요한 공백을 제거합니다.\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    sentence = sentence.lower() #1\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,]+\", \" \", sentence) #2\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #3\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #4\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34f6349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, 정제하여 토큰화를 진행\n",
    "# 최종적으로 ko_tokenizer 과 en_tokenizer 를 얻기\n",
    "# en_tokenizer에는 set_encode_extra_options(\"bos:eos\") 함수를 실행해 타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게\n",
    "# 단어 사전을 매개변수로 받아 원하는 크기의 사전을 정의할 수 있게 합니다. (기본: 20,000)\n",
    "# 학습 후 저장된 model 파일을 SentencePieceProcessor() 클래스에 Load()한 후 반환합니다.\n",
    "# 특수 토큰의 인덱스를 아래와 동일하게 지정합니다.\n",
    "# <PAD> : 0 / <BOS> : 1 / <EOS> : 2 / <UNK> : 3\n",
    "\n",
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성\n",
    "def generate_tokenizer(corpus, vocab_size, lang=\"en\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "\n",
    "    temp_file = os.getenv('HOME') + f'/aiffel/data/corpus_{lang}.txt'     # corpus를 받아 txt파일로 저장\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    # Sentencepiece를 이용해 \n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
    "        --unk_id={unk_id} --model_prefix=spm{lang}_r2 --vocab_size={vocab_size}'   # model_r1\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'spm{lang}_r2.model') # model_r1\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb639b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/data/corpus_en.txt --pad_id=0 --bos_id=1 --eos_id=2         --unk_id=3 --model_prefix=spmen_r2 --vocab_size=25000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/data/corpus_en.txt\n",
      "  input_format: \n",
      "  model_prefix: spmen_r2\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 25000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/data/corpus_en.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=10661485\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9909% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999909\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78956 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 82992 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78956\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 44562\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 44562 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34535 obj=9.86221 num_tokens=83351 num_tokens/piece=2.41352\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25851 obj=8.00619 num_tokens=83809 num_tokens/piece=3.242\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spmen_r2.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spmen_r2.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/data/corpus_ko.txt --pad_id=0 --bos_id=1 --eos_id=2         --unk_id=3 --model_prefix=spmko_r2 --vocab_size=25000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/data/corpus_ko.txt\n",
      "  input_format: \n",
      "  model_prefix: spmko_r2\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 25000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/data/corpus_ko.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5053323\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1185\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78967 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 159138 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78967\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 195706\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 195706 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=83228 obj=12.5937 num_tokens=378608 num_tokens/piece=4.54905\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=70410 obj=11.4417 num_tokens=379922 num_tokens/piece=5.39585\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=52802 obj=11.4471 num_tokens=396861 num_tokens/piece=7.51602\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=52784 obj=11.4136 num_tokens=397192 num_tokens/piece=7.52486\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=39588 obj=11.5538 num_tokens=420665 num_tokens/piece=10.6261\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=39588 obj=11.5172 num_tokens=420679 num_tokens/piece=10.6264\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=29691 obj=11.7107 num_tokens=446988 num_tokens/piece=15.0547\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=29691 obj=11.6693 num_tokens=446992 num_tokens/piece=15.0548\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=27500 obj=11.7272 num_tokens=453579 num_tokens/piece=16.4938\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=27500 obj=11.7164 num_tokens=453580 num_tokens/piece=16.4938\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spmko_r2.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spmko_r2.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 25000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair[0], pair[1]\n",
    "    # kor, eng 나눠서 데이터 정제 후 분리\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, SRC_VOCAB_SIZE, \"en\")\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, TGT_VOCAB_SIZE, \"ko\")\n",
    "ko_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5dfc3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'even the local stock market seems affected by the nasty weather . '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_corpus[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2227bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주식 시장조차 궂은 날씨에 영향을 받은 것으로 보인다 . '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_corpus[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6ec4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 25000\n",
    "\n",
    "eng_corpus_test = []\n",
    "kor_corpus_test = []\n",
    "\n",
    "for pair in cleaned_corpus_test:\n",
    "    k, e = pair[0], pair[1]\n",
    "    # kor, eng 나눠서 데이터 정제 후 분리\n",
    "    kor_corpus_test.append(preprocess_sentence(k))\n",
    "    eng_corpus_test.append(preprocess_sentence(e))\n",
    "\n",
    "# en_test_tokenizer = generate_tokenizer(eng_corpus, SRC_VOCAB_SIZE, \"en\")\n",
    "# ko_test_tokenizer = generate_tokenizer(kor_corpus, TGT_VOCAB_SIZE, \"ko\")\n",
    "# ko_test_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff27dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 611\n",
      "문장의 평균 길이: 135\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eng_corpus 통계\n",
    "\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "\n",
    "for sen in eng_corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(eng_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "#총 max_len의 배열을 만든 후, raw 문장을 돌면서 각 문장별 길이를 sentence_length의 len(sen) 인덱스마다  계속 더해가면서 counting\n",
    "for sen in eng_corpus:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caabaa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 331\n",
      "문장의 평균 길이: 64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kor_corpus 통계\n",
    "\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "\n",
    "for sen in kor_corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(kor_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "#총 max_len의 배열을 만든 후, raw 문장을 돌면서 각 문장별 길이를 sentence_length의 len(sen) 인덱스마다  계속 더해가면서 counting\n",
    "for sen in kor_corpus:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94b6be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_tokens = np.mean(num_tokens) + 2.5 * np.std(num_tokens)\n",
    "# maxlen = int(max_tokens)\n",
    "# print('pad_sequences maxlen : ', maxlen)\n",
    "# print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)*100}%가 maxlen 설정값 이내에 포함됩니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cf30ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fd72d9331b4543b9d3cf39ccc8e1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저를 활용해 토큰의 길이가 50 이하인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축하고, 텐서 enc_train 과 dec_train 으로 변환\n",
    "src_corpus = [] #영어\n",
    "tgt_corpus = [] #한글\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 xxx 이하인 문장만 남깁니다. \n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    src = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    tgt = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    \n",
    "    if len(src) <= 120 and len(tgt) <= 100: \n",
    "        src_corpus.append(src)\n",
    "        tgt_corpus.append(tgt)\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fd84b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 훈련 데이터와 검증 데이터로 분리하기\n",
    "# enc_train, enc_val, dec_train, dec_val = train_test_split(enc_data, dec_data, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9650e838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78967, 116)\n",
      "(78967, 95)\n"
     ]
    }
   ],
   "source": [
    "# enc, dec 의 seq_length는 달라도 상관없음.\n",
    "print(enc_train.shape)\n",
    "print(dec_train.shape)\n",
    "# print(enc_val.shape)\n",
    "# print(dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd4f0da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_tokenizer.encode_as_pieces(eng_corpus[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f534079",
   "metadata": {},
   "source": [
    "# Step 3. 모델설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7fd99",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6aef1c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos - 단어가 위치한 Time-step(각각의 토큰의 위치정보값이며 정수값을 의미)\n",
    "# d_model - 모델의 Embedding 차원 수\n",
    "# i - Encoding차원의 index\n",
    "\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i)/d_model)  # np.power(a,b) > a^b(제곱)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    \n",
    "    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1163382",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a25f609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)  # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        \n",
    "        # Scaled QK 값 구하기\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)\n",
    "        \n",
    "        # 1. Attention Weights 값 구하기 -> attentions\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        # 2. Attention 값을 V에 곱하기 -> out\n",
    "        out = tf.matmul(attentions, V)\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding된 입력을 head의 수로 분할하는 함수\n",
    "        \n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x length x heads x self.depth ]\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "        return split_x\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "        \n",
    "        x: [ batch x length x heads x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "        \"\"\"\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406a3bd",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f5fc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927a7ef",
   "metadata": {},
   "source": [
    "d_ff 는 논문의 설명대로라면 2048 일 거고, d_model 은 512  [ batch x length x d_model ] 의 입력을 받아 w_1 이 2048차원으로 매핑하고 활성함수 ReLU를 적용한 후, 다시 w_2 를 통해 512차원으로 되돌리는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cea52e",
   "metadata": {},
   "source": [
    "## Encoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6399359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual*1 \n",
    "        \n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual*1 \n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec24a80",
   "metadata": {},
   "source": [
    "## Decoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bbce408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        #out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        #out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0b86c",
   "metadata": {},
   "source": [
    "## Encoder와 Decoder 클래스를 정의\n",
    "EncodeLayer 와 DecoderLayer 를 모두 정의했으니 이를 조립하는 것은 어렵지 않겠죠? 이를 이용해 Encoder와 Decoder 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcf9870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "            \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f868e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e7f1c",
   "metadata": {},
   "source": [
    "## Transformer 완성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "600e166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size,\n",
    "                 pos_len, dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        # 1. Embedding Layer 정의\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding 정의\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        # 6. Dropout 정의\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        # 3. Encoder / Decoder 정의\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        # 4. Output Linear 정의\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        # 5. Shared Weights\n",
    "        self.shared = shared\n",
    "        \n",
    "        if shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        \n",
    "        if self.shared:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "        \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        \n",
    "        # Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        # Step 3: Decoder(dec_in, enc_out, mask) -> dec_out, dec_attns, dec_enc_attns\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        # Step 4: Out Linear(dec_out) -> logits\n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6dbbc",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96a8bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d0273",
   "metadata": {},
   "source": [
    "# Step 4. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3eaf1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=6,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.1,\n",
    "    shared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ebe60",
   "metadata": {},
   "source": [
    "##  Learning Rate Scheduler를 선언하고, 이를 포함하는 Adam Optimizer를 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "663f5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d26a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102e5b9",
   "metadata": {},
   "source": [
    "## Loss 함수를 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c226e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1865d4fd",
   "metadata": {},
   "source": [
    "## train_step 함수를 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc6f3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d473413",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer = optimizer , transformer = transformer)\n",
    "manager = tf.train.CheckpointManager(ckpt, './tf_ckpts_gd12_r1',max_to_keep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84ba57",
   "metadata": {},
   "source": [
    "## Attention 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85c92138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4380b",
   "metadata": {},
   "source": [
    "## 번역생성함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2c45c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=enc_train.shape[-1], padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09495f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "728f9ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"How was your day? I was the best.\",\n",
    "    \"Take your time, please.\",\n",
    "    \"I’m about to leave. Please hold for a moment.\",\n",
    "    \"Have you heard of it?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502a954",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2b6f3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def train_and_checkpoint(transformer, manager, EPOCHS):\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    \n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "\n",
    "        idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "        random.shuffle(idx_list)\n",
    "        t = tqdm(idx_list)\n",
    "\n",
    "        for (batch, idx) in enumerate(t):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                                                         dec_train[idx:idx+BATCH_SIZE],\n",
    "                                                                         transformer,\n",
    "                                                                         optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "      \n",
    "      \n",
    "        # 매 Epoch 마다 제시된 예문에 대한 번역 생성\n",
    "        for example in examples:\n",
    "            translate(example, transformer, en_tokenizer, ko_tokenizer)\n",
    "            \n",
    " \n",
    "        if int(ckpt.step) % 2 == 0:\n",
    "            save_path = manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9b08505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25801f94ba804dac93d4561d961b5290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 그는 이 같은 날을 어떻게 사랑했다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 그는 시간 동안 돈을 받게 될 것입니다 .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 그는 일 오후 시 분의 한 남자를 타고 있었다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 그는 나는 나는 나는 내가 어떻게 어떻게 어떻게 어떻게 어떻게 어떻게 어떻게 어떻게 어떻게 어떻게 어떻게 합니다 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e82bdbd4f0843b5a7b672a09be99cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 얼마나 많은 사람들이 얼마나 나를  ⁇ 니다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 그러나 당신은 당신에게 더 빨리를 즐길 수 있다 .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 그는 또는 사람들이 나를 치며 내가 나를  ⁇ 다 고 덧붙였다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 내가 ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d19cdc78084cedae74143651fe8590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 원문기사보기\n",
      "Input: Take your time, please.\n",
      "Predicted translation: to be the u . s . s . s .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 나는 나는 나는 나는 나는 다시한번 땅을 잃고 있다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 내가 원하는 것은 아니다 .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcdebc0a5454111b1e4880b40360ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 어떻게든지에 대해 어떻게 말했는지는 모른다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 시간이 필요하면 시간이 지났을 수도 있습니다 .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 그는 일 현지시간 열린 경기에 출전해 다시 이곳에 왔지만 나는 내가 잊을 수 없다 고 말했다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 내 블로그에 저장\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8361f4ad3bfe467496658921327f8b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 그는 하루 평균 번의 날을 받았다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 필요하면 시간이 일을  ⁇ 다 .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 나는 그를 위해 기도를 해야한다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 내가 우리에게요 .\n"
     ]
    }
   ],
   "source": [
    "train_and_checkpoint(transformer, manager, EPOCHS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "183e4680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 그는 하루 평균 번의 날을 받았다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 필요하면 시간이 일을  ⁇ 다 .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 나는 그를 위해 기도를 해야한다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 내가 우리에게요 .\n"
     ]
    }
   ],
   "source": [
    "for example in examples:\n",
    "    translate(example, transformer, en_tokenizer, ko_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4797ddfa",
   "metadata": {},
   "source": [
    "# Step 5. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aff2a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def eval_bleu_single(model, src_sentence, tgt_sentence, en_tokenizer, ko_tokenizer, verbose=True):\n",
    "    src_tokens = en_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = ko_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > 120): return None\n",
    "    if (len(tgt_tokens) > 100): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_sentence, model, en_tokenizer, ko_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22089182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: that s according to a producer who worked with the actress . \n",
      "Predicted translation: 이는 영화의 홍보를 위해 할 수 있는 한 여배우가 이루어졌다 .\n",
      "Source Sentence:  that s according to a producer who worked with the actress . \n",
      "Model Prediction:  ['이는', '영화의', '홍보를', '위해', '할', '수', '있는', '한', '여배우가', '이루어졌다', '.']\n",
      "Real:  ['이', '소식은', '이', '여배우와', '함께', '일했던', '한', '프로듀서로부터', '나왔습니다', '.']\n",
      "Score: 0.022417\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.022416933501922302"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single test\n",
    "test_idx = 2\n",
    "\n",
    "eval_bleu_single(transformer, \n",
    "                 eng_corpus_test[test_idx], \n",
    "                 kor_corpus_test[test_idx], \n",
    "                 en_tokenizer, \n",
    "                 ko_tokenizer, \n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6c40faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, en_tokenizer, ko_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], en_tokenizer, ko_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d6e97e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d411ca19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3068afa329c44e1bf2c4fa76c322da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  i think we are coming to a moment at which the palestinian authority and the state of israel will undertake serious issues that will take us finally to a point at which we have never been before , olmert added . \n",
      "Predicted translation: 올메르트 총리는 이스라엘의 팔레스타인 영토인 에후드 올메르트와의 관계를 정상화하기 위해 노력하고 있다 며 이스라엘에 심각한 문제가 될 것 이라고 밝혔다 .\n",
      "Input: peter robinson , northern ireland s first secretary , tells sky news people do not want to go back to the time of the troubles \n",
      "Predicted translation: 영한사전 약어표 한영사전 약어표\n",
      "Input: that s according to a producer who worked with the actress . \n",
      "Predicted translation: 이는 영화의 홍보를 위해 할 수 있는 한 여배우가 이루어졌다 .\n",
      "Input: the plaster cast was fitted at fairfield general hospital in bury , near manchester , two months ago after talks broke his right leg when he fell whilst walking his dog . \n",
      "Predicted translation: 그는 개월 전 세계 랭킹 위인 파멜라 홀타니가 병원에서 치료를 받았다 .\n",
      "Input: jumped cents to . a barrel after setting a high of . . \n",
      "Predicted translation: 센트는 센트 올랐다 .\n",
      "Input: while the human rights group has run separate campaigns about web repression and the jailing of net dissidents before now , irrepressible . info will bring them all together . \n",
      "Predicted translation: 인권 보호 단체인 국제사면위원회는 반체제 인사들이 참여한 반면 , 인권 단체는 이들은 달갑지 않은 채 다른 반체제 인사들을 위해 경쟁을 전개할 것이라고 밝혔다 .\n",
      "Input: painting the inside of a house\n",
      "Predicted translation: 현재 하나의 작품 중 하나의 작품 중 하나는 직접 작품의 손에 들고 있는 집 내부의 작품 중 하나의 작품입니다 .\n",
      "Input: icahn s takeaway , according to his open letter , is that an acquisition of yahoo is too financially risky for microsoft given the poor performance of yahoo s board . \n",
      "Predicted translation: 한편 야후의 마이크로소프트 ms 회장은 일 현지시간 뉴욕의 소프트웨어 인수제안을 거절한 바 있다 .\n",
      "Input: the agency made several substantial revisions to the proposed regulations it unveiled in december . \n",
      "Predicted translation: 한편 이번 법안은 지난해 월부터 년 월까지 이어질 예정이다 .\n",
      "Input: both cambodia and thailand lay claim to the th century temple , which sits atop a cliff on cambodian soil but has its most accessible entrance on the thai side . \n",
      "Predicted translation: 캄보디아와 캄보디아를 방문한 캄보디아 최초 캄보디아 법원은 캄보디아의 마을을 통제하고 있다 .\n",
      "Input: in the interview , pelosi said the president was in no position to criticize congress and brushed aside the criticisms as something to talk about because he has no ideas . \n",
      "Predicted translation: 한편 , 인터뷰를 통해 부시 대통령은 이러한 주장에 대해 의회가 공식적인 입장을 표명하는 것이 아니라는 점을 지적하고 싶지 않다고 말했다 .\n",
      "Input: the president said that as the constitutional head of the armed forces , he takes full political responsibility for what he described as a slip up . \n",
      "Predicted translation: 대통령은 헌법 재판소에서 이 같은 결정이 헌법을 통해 모든 것은 헌법이 잘못되고 있다 며 이를 통해 이를 통해 모든 것이 무엇보다 중요하다 고 말했다 .\n",
      "Input: bbc reports from communist china say a man has been shot dead after taking australian tourists hostage . \n",
      "Predicted translation: bbc는 중국의 무기 중아이던 중 젊은이 사망했다고 보도했다 .\n",
      "Input: japanese prime minister shinzo abe said yesterday japan will accelerate its move to develop a missile defense system in response to north korea s nuclear provocations . \n",
      "Predicted translation: 아베 신조 일본 관방장관은 일본 관방장관은 북한이 미사일 발사를 준비 중이라고 미사일 발사를 준비 중이라고 미사일 방어 대응에 대응하도록 강요할 것이라고 밝혔다 .\n",
      "Input: first class ehud goldwasser and staff sgt . eldad regev , sparked the day war between israel and hezbollah two years ago . \n",
      "Predicted translation: 우선 영은 kg으로 이스라엘 남부와 글래스고에서 열린 레바논의 전쟁을 막기 위해 이스라엘로 들어가는 것을 제안했다 .\n",
      "Input: the man who gunned down people in an immigrant center last week apparently wrote a letter to a television station in upstate new york . \n",
      "Predicted translation: 세의 남자는 뉴욕에서의 남자를 향해 새 앨범을 쓴 세의 남자를 경찰에 고소했다 .\n",
      "Input: the paint will just take a bit longer to dry . \n",
      "Predicted translation: 이 페인트는 더 이상 오래된 페인트가 칠해진 성분을 담은다 .\n",
      "Input: the group is characterized as one of the most comfortable , along with group d in which mexico , portugal , iran and angola will compete . \n",
      "Predicted translation: 이 단체는 이란의 최고 경쟁을 벌여 이란의 최대 야당인 쿠다 .\n",
      "Input: mccain supports the agreement . \n",
      "Predicted translation: 매케인은 매케인을 지지하고 있다 .\n",
      "Input:  months . \n",
      "Predicted translation: 몇 달이는 비행기니 .\n",
      "Input: the scale of the event was revealed when pilgrims arrived in droves and gathered along a waterfront tuesday near the landmark sydney harbor bridge for a twilight mass , beginning with a procession of groups from countries waving their national flags . \n",
      "Predicted translation: 한편 일 오전 시 분에는 아랍축구연맹 a league national 으로 향하던 곳을 방문 , 처음으로 매년 깃발을 흔들던 중 인근서 발견됐다 .\n",
      "Input: the two countries agreed to meet monday to defuse tensions even as each side continued to amass more troops to the site of the preah vihear temple . \n",
      "Predicted translation: 양국은 일 현지시간 두 나라를 만날 수 있는 반면 , 두 나라는 긴장을 고조시킨 것에 합의했다 .\n",
      "Input: the men are recovering at the german embassy in the turkish capital , ankara , according to germany s foreign minister . \n",
      "Predicted translation: 터키 앙카라에서는 터키의 수도 앙카라에서 있었으며 터키의 터키 수도 터키 수도 터키 수도 런던에서도 해외 순방을 할 예정이라고 터키 외무부가 밝혔다 .\n",
      "Input: riots in pakistan spurred by the publication in europe of caricatures of the prophet muhammad spilled over to a south korean transportation company in peshawar on wednesday . \n",
      "Predicted translation: 일 그리스 국경을 넘던 파키스탄의 한 야당이 유럽축구연맹 uefa 으로부터 오는 일 현지시간 이 개를 공개했다 .\n",
      "Input: the dow rallied back after ending tuesday at its lowest level since july , . \n",
      "Predicted translation: 다우존스 산업평균 지수는 일 현지시간 반등에 실패 , 년 만에 최저치를 기록했다 .\n",
      "Input: it called lee s proposal a deceitful tactic to avoid taking responsibility for strained ties . \n",
      "Predicted translation: 이 협정은 이 제안을 확실히 하는 것 을 요구하는 이 제안을 한 바쳤다 .\n",
      "Input: in it , jiverly wong claimed police had been harassing him for years , even touching him in his sleep . \n",
      "Predicted translation: 그는 자신의 뜻대로 도쿄를 떠나면서도 잠을 걷거나 정신적으로 잠들어 있었다 .\n",
      "Input: the land is valued at . billion won . \n",
      "Predicted translation: 이 가치는 억 천만 달러에 이르며 , 가치는 억 달러가 소요될 것이다 .\n",
      "Input: poland s political establishment still has to sign off on the deal and determine the next steps , the official said . \n",
      "Predicted translation: 한편 , 현재는 다른 국가들은 아직까지 협상을 진행하고 있다 .\n",
      "Input: mughniyeh , in his late s , had been variously described as special operations or intelligence chief of hezbollah s secretive military wing , the islamic resistance . \n",
      "Predicted translation: 무그니예는 지난달 일에도 아프간 정보부와의 합동 군사 정보 제공에 대해 분개했다 .\n",
      "Input: the draft u . n . resolution expresses deep concern at the gross irregularities during the june presidential election , the violence and intimidation perpetrated in the run up to the election that made impossible the holding of free and fair elections , and the creation of an environment that did not permit international election observers to operate freely before the june vote . \n",
      "Predicted translation: 유엔 안보리는 지난해 월 일 대선을 공식 선언했다 . 총선과 총선의 투표에 참여하도록 국제사회가 대립하고 있는 총선의 투표에 대해 국제사회의 대립을 일으키지 않으며 대선을 공식석은 해결하지 않고 있다 .\n",
      "Input: seoul , south korea cnn south korea will cut back its regular military exercises as part of a government campaign to save energy amid surging global oil prices , the defense ministry said monday . \n",
      "Predicted translation: 조선의 석유가격 급등은 에너지 , 에너지 , 전력 및 에너지 보급로의 환멸을 위해 일 현지시간 군을 위해 전력을 다로  ⁇ 힐 예정이라고 일 현지시간 국방부가 밝혔다 .\n",
      "Input: the us trade deficit plunged nearly from december to january , but the government reports that s only because the recession drove down american demand for oil and other imported goods . \n",
      "Predicted translation: 미국 무역부는 지난 월 말 , 미국산 쇠고기 수입 재개를 위해 미국산 쇠고기를 추가로 지불하는 것이 아니라며 , 다른 정부의 요구를 거절했습니다 .\n",
      "Input: asked what iran would do to control prices , he replied by the supply , the market . \n",
      "Predicted translation: 아마디네자드 대통령은 이란의 핵 연료 시장을 통제하기 위해 시장을 통제할 것이라며 이란이 핵무기를 개발할 것이라 밝혔다 .\n",
      "Input: chemically damaged and burnt remains found outside the city of yekaterinburg in are those of crown prince alexei , , the last emperor s only son and heir to the throne , and his sister grand duchess maria , about , according to the investigative committee of the russian prosecutor general s office . \n",
      "Predicted translation: 한편 알베턴에 본사를 둔 알렉스 코흐리포트의 에드 지는 이유는 내가 아들과 둘러 싸인 채동이 있는 것으로 드러났다 .\n",
      "Input: speaking to a closed fundraiser in san francisco , california , in early april , obama said decades of lost jobs and unfulfilled promises from washington have left some pennsylvanians bitter and clinging to guns or religion or antipathy to people who aren t like them , or anti immigrant sentiment or anti trade sentiment as a way to explain their frustrations . \n",
      "Predicted translation: 워싱턴 d . c는 미국 워싱턴의 한 백화점에서 오바마 당선인이 미국 캘리포니아 주 전체에서 오바마 후보를 선정하는 것을 포함해 그들이 책임을 져야 한다고 강조했다 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  cnn police in western china shot and killed five people in a group that was planning a holy war against han chinese , the largest ethnic group in china , police told china s xinhua news agency wednesday . \n",
      "Predicted translation: 중국 관영 언론은 일 현지시간 중국 남서부에서 명을 사살했다고 밝혔다 .\n",
      "Input: we re worried about battery life some early reviews indicate that the iphone g lasts only a day but we ll run full tests over the next couple of days and report our results on this page . \n",
      "Predicted translation: 그는 마지막날 아침 에는  ⁇ 막에 대해 좀더 자세하게 나타나길 바란다 며 그러나 결과는 특히 새가 될 것 이라고 덧붙였다 .\n",
      "Input: the connecticut based company building the orion toilet needs the large volume of urine about the daily output of people to work on urine acidity problems , said spokesman leo makowski . \n",
      "Predicted translation: 영국 내무부 대변인은 자체 개발한 변기를 인용해 자사의 상황이 민감해질 수 있다는 사실 이라며 이를 통해 문제가 있는 모든 정보를 공개할 수 있도록 도울 것 이라고 밝혔다 .\n",
      "Input: citing the high prices americans are paying at the pump , bush said from the white house rose garden that allowing offshore oil drilling is one of the most important steps we can take to reduce that burden . \n",
      "Predicted translation: 부시 대통령은 백악관에서 열린 기자회견에서 고유가는 중요한 문제를 해결할 수 있는 중요한 문제라고 말했다 .\n",
      "Input: the dalai lama , who lives in exile in india , insists he has no political role and played no part in the protests by tibetan buddhist monks that erupted into rioting in the main city lhasa last month . \n",
      "Predicted translation: 망명중인 달라이라마는 지난 년대 티베트의 시위를 벌이고 있는 달라이라마와 티베트의 시위를 벌이는 것을 지시하며 시위를 벌였다 .\n",
      "Input: studies show productivity drops when temperatures dip below degrees . \n",
      "Predicted translation: 연구 성과를 토대로 하는 연구에서 온도는 도축을 시작했다 .\n",
      "Input: still , even without the bonuses , rodriguez has a major league high million salary this year . \n",
      "Predicted translation: 그럼 에도 불구하고 , 이는 지난해 천만 달러의 보상금을 받는 조건으로 억 천만 달러의 보상금을 받고 있다 .\n",
      "Input: the findings from the pew internet and american life project challenge the argument that broadband providers need to more aggressively roll out supply to meet demand . \n",
      "Predicted translation: 이 연구결과는 우리가 직접 통하는 많은 고객들이 도전을 받고 , 그들이 오히려 더 많은 양의 양을 지킬 수 있도록 하는 것이 더 중요하다고 주장한다 .\n",
      "Input: ronaldinho arrived at ac milan on wednesday to join kaka and alexandre pato in an all brazilian attack that ac milan are convinced will fire them back to the top level of european football . \n",
      "Predicted translation: 브라질선수협회는 일 현지시간 브라질리아 a조 경기에서 유럽축구연맹 uefa 챔피언스리그에서 브라질과의 유럽 선수들을 위해 유럽을 순방했다 .\n",
      "Input: amnesty is celebrating years of activism by highlighting governments using the net to suppress dissent . \n",
      "Predicted translation: 인도는 그동안 소수의 악인들의 반대를 선동하기 위해 나를 신이 영향력을 행사하는 것을 강하게 비난했다 .\n",
      "Input: only people survived , and more than bodies had been recovered . \n",
      "Predicted translation: 한편 , 명의 사망자는 명에 이르는 것으로 밝혀졌다 .\n",
      "Input: the cabinet reshuffle puts new ministers in charge of agriculture , welfare and education affairs , a presidential spokesman said , according to yonhap . \n",
      "Predicted translation: 새 내각 개편안은 책임지고 , 복지 담당자가 , 복지 및 복지 센터를 관리하도록 하는 것이라고 연합통신이 보도했다 .\n",
      "Input:  it s going to have to be a process of getting people more engaged with information technology and demonstrating to people it s worth it for them to make the investment of time and money . \n",
      "Predicted translation: 이는 더 많은 사람들이 더 많은 정보를 교환하고 , 사람들이 더 많은 사람들에게 정보를 교환하고 , 사람들이 더 나을 수 있는 정보를 교환하는 데에 환하게 될 것이다 .\n",
      "Input: russia fervently opposes basing the interceptors right across its border and says the system s real target would be russian missiles , according to time magazine . \n",
      "Predicted translation: 러시아는 최근 로마 md md 시스템과 미사일을 배치할 수 있는 최신 미사일 시험 발사를 계획하고 있다고 밝혔다 .\n",
      "Input: through a request filed under the freedom of information act , fishwick s firm was able to obtain records that showed the virginia state lottery sold million in tickets for which no top prize was available . \n",
      "Predicted translation: 이 복권은 버지니아주의 두번째 주가 조작으로 만달러 약 억원 의 보상금을 요청했다 .\n",
      "Input: the fanatical groups exist in the vacuum of middle eastern politics , nurtured by the rot of its illegitimate dictatorships and the heretical extremism of its mullahs and clerics . \n",
      "Predicted translation: 이런 가운데 우리는 소수의 민주주의와 법치 않다 . 그들은 법으로 지배되는 것이며 , 법인 지배층을 형성하고 있다 .\n",
      "Input: iran wednesday hinted at a possible increase in its production of crude oil to stabilize prices as they hovered above a barrel . \n",
      "Predicted translation: 이란은 일 현지시간 국제유가가 배럴당 달러 선을 돌파해 배럴당 달러를 돌파한 것에 대해 비난했다 .\n",
      "Input:  there is no excuse for delay , the president said in a rose garden statement last month . \n",
      "Predicted translation: 성명은 지난달 성명에서 로즈 가든에서 성명을 발표하는 동안 지연된 상태는 없었다고 전했다 .\n",
      "Input: the captain of south korean ships released after over five months in captivity said sunday some of his crew members were suffering from physical pain and are still in a state of fear . \n",
      "Predicted translation: 한국 군당국은 일 현지시간 한국 선박에 실려 있던 배가 넘는 배가 넘는 배가 넘는 배가 발견되는 것에 대해 그의 사인은 아직까지 일부가고 있다 .\n",
      "Input: santa barbara , california cnn authorities arrested a year old homeowner for setting backfires without permission as firefighters continue to battle two of california s most threatening wildfires . \n",
      "Predicted translation: 산타 바바라 당국은 이 용의자가 캘리포니아 주 관할 당국을 통해 이웃 주민 명을 태우고갔다 .\n",
      "Input: now former untouchable women from alwar are getting their wish . \n",
      "Predicted translation: 현재 여성은 자신의 전 여성들이 여성들이 여성들이 필요하다 .\n",
      "Input: among those to brave the tv cameras at the ronald reagan presidential library in simi valley , california , are rudi giuliani , john mccain and mitt romney . \n",
      "Predicted translation: 한편 , 캘리포니아 주와 뉴저지 , 메사추세츠 , 캘리포니아 , 레스코 , 존 맥케인 의원과 , 캘리포니아 , 레볼드 , 스코크는 각각와 존 맥케인이 캘리포니아에서도 공격하고 있다 .\n",
      "Input: despite the increase in overall broadband adoption , though , growth has been flat among blacks and poorer americans . \n",
      "Predicted translation: 비든 국장은 광대역결편은 미국 가정에는 비결되는 것이며 , 미국으로부터의 성장률을 기록했다 .\n",
      "Input: toronto pitcher roy halladay earned the biggest all star bonus , , , followed by cleveland pitcher cliff lee at , . \n",
      "Predicted translation: 토론토 토론토 클리블랜드 관할 경찰은 각각 대와 대를 얻었다 .\n",
      "Input: bogota , colombia cnn freed colombian hostage ingrid betancourt tearfully embraced her children on thursday for the first time in more than six years , a day after being rescued from leftist rebels who kept her captive in the jungles of colombia . \n",
      "Predicted translation: 콜롬비아무장혁명군 farc 에 의해 피랍된 중남미 지도자 아웅산수치 여사가 일 현지시간 farc와 피랍된 한국인 인질을 석방했다 .\n",
      "Input: united chief executive david gill believes that could have been a tactic to command a higher salary at old trafford . \n",
      "Predicted translation: 데이비드  ⁇ 컴 회장은 이 단체가 이 같은 명령을 내린 것으로 생각된다는 점을 강조했다 .\n",
      "Input: midwest airlines released a statement saying that while there was never an issue as to the safety of the flight , as a precautionary measure , we decided to divert the plane . \n",
      "Predicted translation: 한편 필라델피아에서 비행기를 탑승한 항공사 소속 비행기는 안전하게 이륙할 당시 안전모절단을 계획했다고 밝혔다 .\n",
      "Input: most of the players win through the second , third or fourth place prizes , she said . \n",
      "Predicted translation: 그는 또 번째 선수는 번째 선수로 선정되지 않았다 .\n",
      "Input: light , sweet crude for august delivery added cents to . on the new york mercantile exchange . \n",
      "Predicted translation: 뉴욕 상업거래소 nymex 에서 거래된 월 인도분 서부텍사스 중질유 wti 는 센트 오른 . 달러에 거래됐다 .\n",
      "Input: rosenberg denied reports that spears , , is shooting a music video . \n",
      "Predicted translation: 그러나 스피어스는 음악과 관련 , 동영상을 보여주고 있다고 밝혔다 .\n",
      "Input:  i just feel blessed to have played here . \n",
      "Predicted translation: 나는 평범한 세대를 위해 노력했다 .\n",
      "Input: ideal temperature \n",
      "Predicted translation: corage\n",
      "Input: johnson s eviscerated body which police said they found after receiving calls about a foul odor coming from the apartment was in a state of moderate decomposition , and she had been dead about two days , medical examiner karl williams said . \n",
      "Predicted translation: 존슨은 캠벨 박사가 자신의 아파트에서 치인류를 발견한 후 그를 발견했다고 밝혔으며 그의 시신은 아직까지 발견되지 않았다고 전했다 .\n",
      "Input: the asteroids are thought to be the leftovers after the planets were made . \n",
      "Predicted translation: 이 소행성은 종의 궤도를 향해 자극을 받을 수 있는 것이었다 .\n",
      "Input: however , the confidential military source who showed cnn the photographs that included the man wearing the bib said they were taken moments before the mission took off . \n",
      "Predicted translation: 그러나 군 소식통이 입수한 군당국과 관련 소식통은 사진의 사진을 찍어 ⁇ 으로써 사진 속삭발사에 놓고 있다고 전했다 .\n",
      "Input: even though he figured his chances of winning were a long shot , he felt the odds were reasonable . \n",
      "Predicted translation: 그는 결국 그는 결국 신에게 기회를 상실했다 며 그는 왼쪽 날개가 달린  ⁇ 은 아니었다 고 덧붙였다 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: her doctor also said on monday that fawcett is recovering after a medical procedure in germany , and is not on death s door . \n",
      "Predicted translation: 그녀의 주치의인 의사는 일 현지시간 주치의가 생후 개월 만에 환자의 상태를 조사하기 위해 소송을 청구했다고 전했다 .\n",
      "Input: turkish police have detained four people in connection with wednesday s terrorist attack outside the u . s . consulate in istanbul , the interior minister said thursday . \n",
      "Predicted translation: 터키의 경찰은 일 현지시간 이스탄불 중심부를 점령하고 있는 터키 이스탄불에 있는 이스탄불 중심부를 상대로 폭탄 테러를 저질렀다고 밝혔다 .\n",
      "Input: abe emphasized tokyo will further enhance security cooperation with washington , pledging stern measures against pyongyang . \n",
      "Predicted translation: 아베 총리는 , 일본 , 체코와의 협력을 위한 협력을 강화할지도 모른다 .\n",
      "Input: coots was charged thursday with buying , selling and possessing illegal reptiles . \n",
      "Predicted translation: 한편 농심은 일 현지시간 불법 무기를 구입하고 판매하고 불법으로 판매하는 것을 요구했다 .\n",
      "Input: clemens contends mcnamee defamed him when the trainer , who worked for both pitchers , told pettitte in or that clemens had used hgh and in or that clemens had used steroids . \n",
      "Predicted translation: 한편 클레멘스는 이날 경기를 치른 후 맥나미가 거짓말을 하고 있으며 그는 맥나미가 거짓말을 한 후 그를 대신해 검사한 적이 없으며 그를 살리거나 그를 살리지 않았다고 말했다 .\n",
      "Input: the us needs to move more exports for the overall economy to improve . \n",
      "Predicted translation: 수출은 , 보다 더 많은 사람들을 수출하는 것을 보다 엄격한 조치를 취할 것입니다 .\n",
      "Input: one of the robbery suspects escaped , owino said . \n",
      "Predicted translation: 강도 . 테러 용의자 중 명은 도주 중이라고 그의 말을 인용해 보도했다 .\n",
      "Input: turkey , for instance , or jordan or morocco\n",
      "Predicted translation: 터키 , 혹은 혹은 혹은 지난달 일 터키를 방문했다 .\n",
      "Input: in an interview with ok ! \n",
      "Predicted translation: 그는 인터뷰를 다 ?\n",
      "Input: total had been expected to help develop a huge liquefied natural gas project linked to iran s south pars gas field with malaysia s petronas . \n",
      "Predicted translation: 이란은 이로써 이란과의 협력을 목적으로 하는 건설회사인 베트남의 최대 가스와 유전 가스를 생산하지 못하고 있다 .\n",
      "Input: last minute negotiations between the pilots union and the management failed on the th day of the strike , pushing the government to intervene in the dispute to prevent the strike from inflicting more damage to the export industry and inconveniencing passengers during the peak summer holiday season . \n",
      "Predicted translation: 이날 마지막 협상은 항공과 정부의 공식적인 충돌을 막기 위해 만 달러가 소요되는 것을 막기 위해 총력을 기울이고 있다 .\n",
      "Input: the sea blanketing algae bloom , which officials blamed on natural causes , had disrupted practice for the more than olympic sailing teams already there . \n",
      "Predicted translation: 관계자들은 이 선박이 터픈 원인을 파악하는데 어려움을 겪고 있다 .\n",
      "Input: the olympic games open august in beijing , and the sailing competition off qingdao begins the next day . \n",
      "Predicted translation: 한편 올림픽 주경기장은 오는 일 열리는 올림픽 주경기장에서도 알리와현상이 열려 있다 .\n",
      "Input: his open defiance of the republican establishment has helped to bolster his image as a maverick lawmaker willing to shun party loyalty and obedience for personal principle . \n",
      "Predicted translation: 그는 자신의 제안을 하지 않았고 , 자신이 직접 이미지를 존중하고 , 자신의 이미지를 존중하는 데에 도움이 되는 것을 막기 위해 필요한 조치를 취했다 .\n",
      "Input: but a president will swear in whoever is picked as the new prime minister . \n",
      "Predicted translation: 하지만 , 신인 수상은 새 총리를 지명할 것입니다 .\n",
      "Input: baghdad , iraq cnn presumptive u . s . democratic presidential candidate barack obama arrived in iraq on monday for talks with iraqi officials and american military commanders in a year old war he has pledged to end , a u . s . embassy spokesman said . \n",
      "Predicted translation: 이라크 수도 바그다드에서 일 현지시간 미군의 이라크 반군에 대한 미국의 이라크 주둔 미군을 위해 이라크 주둔 미군을 방문했다 .\n",
      "Input: it is unclear what prompted the rebels to release the hostages . \n",
      "Predicted translation: 저장된 검색어가 없습니다 .\n",
      "Input: stockholm , sweden cnn swedish officials say a theme park ride has collapsed in a city in western sweden , injuring people . \n",
      "Predicted translation: 스웨덴 빅토리아 시 스웨덴 시내 스웨덴 스웨덴 경찰은 스웨덴 스웨덴 북부 스웨덴 노라카와 스웨덴에서 스웨덴으로 구성된 스웨덴 스 ⁇ 크를 스웨덴 스웨덴과 스웨덴에 있는 스웨덴 경찰 명을 태우고 스웨덴 스웨덴 스웨덴 스웨덴 경찰 명이 부상했다고 전했다 .\n",
      "Input: in a fundraising e mail to supporters , obama campaign manager david plouffe acknowledged the deficit , saying mccain and the rnc together still have a huge cash advantage , and we need your help to close the gap . \n",
      "Predicted translation: 오바마는 지지자들에게 이에 대해 뉴욕타임스와의 캠페인을 벌이는 힐러리 클린턴 상원의원과 함께 일하는 것은 아니라고 강조했다 .\n",
      "Input:  if he can get back to being the player he was , perhaps we could have a rethink , guardiola said last month . \n",
      "Predicted translation: 그는 만약에 한번도 팀에 복귀할 수 있는 것 이라며 아마도 아마도 아마도 아마도 아마도 없었다 고 말했다 .\n",
      "Input: n disappears , director bayona cranks up the jitters for mother laura . \n",
      "Predicted translation: 한편 교전 중 피살된 상태로 그의 대리인들이 등장으로 보일 수 있다 .\n",
      "Input: the cabinet approved the deal in a to vote . \n",
      "Predicted translation: 이번 내각은 해당 의결을 대신 승인했다 .\n",
      "Input: two other police officers were wounded in the attack . \n",
      "Predicted translation: 경찰은 이 폭탄테러로 경찰관 명과 명이 부상했다 .\n",
      "Input: police said the year old entered the exhibit shortly after the museum doors opened at a . m . and made for the hitler figure , scuffling with a guard assigned to protect it and the manager before tearing the head off the life size statue . \n",
      "Predicted translation: 경찰은 이 남자가잔에 있는 한 보관원을 약탈한 후 보관된 후 보관된 후 보관을 한 뒤 이를 공개해 적발했으며 이 박물관에는 전시됐으며 약탈에 대한 내부를 보호하는데 어려움을 겪고 있다 .\n",
      "Input: light , sweet crude for august delivery rose . to settle at a record . a barrel on the new york mercantile exchange . \n",
      "Predicted translation: 뉴욕 상업거래소 nymex 에서 거래된 월 인도분 서부텍사스 중질유 wti 는 배럴당 . 달러를 기록했다 .\n",
      "Input:  in an interview with the associated press , caroline kennedy goes on the record about her bid for the senate , and acknowledges people will expect more of her because of who she is \n",
      "Predicted translation: 케네디는 인터뷰를 통해 내가 자신이 더 많은 사람들이 내가 하는 것은 더 이상의 순간 이라며 힐러리는 모든 혐의를 받고 있는 사람을 더 신중해야 한다 고 지적했다 .\n",
      "Input: bpi teamed up with virgin media in june to launch an education campaign to stop customers illegally downloading music . \n",
      "Predicted translation: 한편 브라운은 일 빅토리아 시로 예정된 캠페인을 시작했다 .\n",
      "Input: police and prosecutors say they have opened an investigation . \n",
      "Predicted translation: 경찰은 이 사건을 조사 중이라고 밝혔다 .\n",
      "Num of Sample: 100\n",
      "Total Score: 0.023205682551263648\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, eng_corpus_test[:100], kor_corpus_test[:100], en_tokenizer, ko_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bleu example\n",
    "# from datasets import load_metric\n",
    "\n",
    "# bleu = load_metric(\"bleu\")\n",
    "# predictions = [[\"I\", \"have\", \"thirty\", \"six\", \"years\"]] \n",
    "# references = [\n",
    "#     [[\"I\", \"am\", \"thirty\", \"six\", \"years\", \"old\"], [\"I\", \"am\", \"thirty\", \"six\"]]\n",
    "# ]\n",
    "# bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# # Bleu example\n",
    "# from datasets import load_metric\n",
    "\n",
    "# bleu = load_metric(\"bleu\")\n",
    "# #bleu = load_metric('sacrebleu')\n",
    "\n",
    "# reference = [kor_corpus_test.split()]\n",
    "# predictions = [translate(eng_corpus_test, transformer, en_tokenizer, ko_tokenizer).split()]\n",
    "\n",
    "# bleu.compute(predictions=predictions, references=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49993ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "# import matplotlib.font_manager as fm\n",
    "\n",
    "# fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "# font = fm.FontProperties(fname=fontpath, size=9)\n",
    "# plt.rc('font', family='NanumBarunGothic') \n",
    "# # mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매 Epoch 마다 제시된 예문에 대한 번역 생성시각화\n",
    "# for example in examples:\n",
    "#     translate(example, transformer, en_tokenizer, ko_tokenizer, plot_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8887af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbca665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
