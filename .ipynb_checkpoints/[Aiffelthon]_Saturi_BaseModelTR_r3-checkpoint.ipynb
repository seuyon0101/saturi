{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a0371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "# from tqdm import tqdm_notebook \n",
    "from tqdm.notebook import tqdm \n",
    "import random\n",
    "\n",
    "import sentencepiece as spm\n",
    "from konlpy.tag import Mecab\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1fe9a",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42fb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data load\n",
    "data_dir = os.getenv('HOME')+'/aiffel/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\"\n",
    "\n",
    "# test data loading\n",
    "kor_path_test =data_dir+\"/korean-english-park.test.ko\"\n",
    "eng_path_test =data_dir+\"/korean-english-park.test.en\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756a255",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "261257ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
    "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
    "\n",
    "    assert len(kor) == len(eng) # kor, eng가 같은 갯수라는 것을 검증받기 위해 적용\n",
    "\n",
    "    cleaned_corpus = list(set(zip(eng, kor)))  # 중복된 데이터 제거\n",
    "    \n",
    "    return cleaned_corpus\n",
    "\n",
    "cleaned_corpus = clean_corpus(eng_path, kor_path)\n",
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670ae538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test 데이터 정제 및 토큰화\n",
    "def clean_corpus_test(kor_path_test, eng_path_test):\n",
    "    with open(eng_path_test, \"r\") as f: eng_test = f.read().splitlines()\n",
    "    with open(kor_path_test, \"r\") as f: kor_test = f.read().splitlines()\n",
    "\n",
    "    assert len(kor_test) == len(eng_test) # kor, eng가 같은 갯수라는 것을 검증받기 위해 적용\n",
    "\n",
    "    cleaned_corpus_test = list(set(zip(eng_test, kor_test)))  # 중복된 데이터 제거\n",
    "    \n",
    "    return cleaned_corpus_test\n",
    "\n",
    "cleaned_corpus_test = clean_corpus_test(eng_path_test, kor_path_test)\n",
    "len(cleaned_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966caf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('슈마허는 이러한 이야기를 서로 해야 한다고 생각하게 됐다.',\n",
       "  'Schumacher, the sales account manager from San Francisco, would have appreciated having that conversation.'),\n",
       " ('GNP to sue former senior official of the spy agency ˝한나라당, 전 국정원 간부 고발 예정˝\\xa0 \\xa0 \\xa02007.10',\n",
       "  'On Wednesday, the government announced the latest steps in its bid to achieve the financial hub plan, including providing tax breaks to promote mergers and acquisitions (M&As) between financial companies.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "520ffb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('첸 젱핑은 독극물을 사용한 죄로 난징 제1 중재 인민 법원에 의해 선고를 받았다고 관영 신화 통신이 보도했다.',\n",
       "  \"by the Nanjing No.1 Intermediate People's Court for usage of a dangerous substance, the official Xinhua news agency reported.\"),\n",
       " ('만 대변인은 피셔 보엘 위원 앞으로 규칙적으로 보내진 대부분 우유는 배달 중에 상했고 일부는 우유곽이 열려 있었다고 말했다.',\n",
       "  'Most of the milk sent by regular mail and addressed to Fischer Boel had spoiled in transit, and some of the cartons had burst open, he said.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d784123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_corpus+=cleaned_corpus_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c65e1d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78968"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af973b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모든 입력을 소문자로 변환합니다.\n",
    "# 2. 알파벳, 문장부호, 한글만 남기고 모두 제거합니다.\n",
    "# 3. 문장부호 양옆에 공백을 추가합니다.\n",
    "# 4. 문장 앞뒤의 불필요한 공백을 제거합니다.\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    sentence = sentence.lower() #1\n",
    "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,]+\", \" \", sentence) #2\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) #3\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #4\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fb40783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 말뭉치 kor_corpus 와 영문 말뭉치 eng_corpus 를 각각 분리한 후, 정제하여 토큰화를 진행\n",
    "# 최종적으로 ko_tokenizer 과 en_tokenizer 를 얻기\n",
    "# en_tokenizer에는 set_encode_extra_options(\"bos:eos\") 함수를 실행해 타겟 입력이 문장의 시작 토큰과 끝 토큰을 포함할 수 있게\n",
    "# 단어 사전을 매개변수로 받아 원하는 크기의 사전을 정의할 수 있게 합니다. (기본: 20,000)\n",
    "# 학습 후 저장된 model 파일을 SentencePieceProcessor() 클래스에 Load()한 후 반환합니다.\n",
    "# 특수 토큰의 인덱스를 아래와 동일하게 지정합니다.\n",
    "# <PAD> : 0 / <BOS> : 1 / <EOS> : 2 / <UNK> : 3\n",
    "\n",
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성\n",
    "def generate_tokenizer(corpus, vocab_size, lang=\"en\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
    "\n",
    "    temp_file = os.getenv('HOME') + f'/aiffel/data/corpus_{lang}.txt'     # corpus를 받아 txt파일로 저장\n",
    "    \n",
    "    with open(temp_file, 'w') as f:\n",
    "        for row in corpus:\n",
    "            f.write(str(row) + '\\n')\n",
    "    \n",
    "    # Sentencepiece를 이용해 \n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        f'--input={temp_file} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
    "        --unk_id={unk_id} --model_prefix=spm{lang}_r2 --vocab_size={vocab_size}'   # model_r1\n",
    "    )\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load(f'spm{lang}_r2.model') # model_r1\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c1c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/data/corpus_en.txt --pad_id=0 --bos_id=1 --eos_id=2         --unk_id=3 --model_prefix=spmen_r2 --vocab_size=25000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/data/corpus_en.txt\n",
      "  input_format: \n",
      "  model_prefix: spmen_r2\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 25000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/data/corpus_en.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=10661485\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9909% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999909\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78956 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 82992 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78956\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 44562\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 44562 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34535 obj=9.86221 num_tokens=83351 num_tokens/piece=2.41352\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25851 obj=8.00619 num_tokens=83809 num_tokens/piece=3.242\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spmen_r2.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spmen_r2.vocab\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/data/corpus_ko.txt --pad_id=0 --bos_id=1 --eos_id=2         --unk_id=3 --model_prefix=spmko_r2 --vocab_size=25000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/data/corpus_ko.txt\n",
      "  input_format: \n",
      "  model_prefix: spmko_r2\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 25000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/data/corpus_ko.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78968 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5053323\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1185\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78967 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 159138 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78967\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 195706\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 195706 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=83228 obj=12.5937 num_tokens=378608 num_tokens/piece=4.54905\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=70410 obj=11.4417 num_tokens=379922 num_tokens/piece=5.39585\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=52802 obj=11.4471 num_tokens=396861 num_tokens/piece=7.51602\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=52784 obj=11.4136 num_tokens=397192 num"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_tokens/piece=7.52486\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=39588 obj=11.5538 num_tokens=420665 num_tokens/piece=10.6261\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=39588 obj=11.5172 num_tokens=420679 num_tokens/piece=10.6264\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=29691 obj=11.7107 num_tokens=446988 num_tokens/piece=15.0547\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=29691 obj=11.6693 num_tokens=446992 num_tokens/piece=15.0548\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=27500 obj=11.7272 num_tokens=453579 num_tokens/piece=16.4938\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=27500 obj=11.7164 num_tokens=453580 num_tokens/piece=16.4938\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spmko_r2.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spmko_r2.vocab\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 25000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair[0], pair[1]\n",
    "    # kor, eng 나눠서 데이터 정제 후 분리\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, SRC_VOCAB_SIZE, \"en\")\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, TGT_VOCAB_SIZE, \"ko\")\n",
    "ko_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fe9bad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_corpus:  the ferry s manifest showed passengers and crew were on board the boat , but one official said the real number was higher . \n",
      "kor_corpus:  여객선 승객명단을 보면 명의 승객과 승무원이 타고 있었으나 한 당국자에 의하면 실제 인원은 더 많을 것으로 여겨진다 . \n"
     ]
    }
   ],
   "source": [
    "print('eng_corpus: ', eng_corpus[100])\n",
    "print('kor_corpus: ', kor_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a8ede27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 25000\n",
    "\n",
    "eng_corpus_test = []\n",
    "kor_corpus_test = []\n",
    "\n",
    "for pair in cleaned_corpus_test:\n",
    "    k, e = pair[0], pair[1]\n",
    "    # kor, eng 나눠서 데이터 정제 후 분리\n",
    "    kor_corpus_test.append(preprocess_sentence(k))\n",
    "    eng_corpus_test.append(preprocess_sentence(e))\n",
    "\n",
    "# en_test_tokenizer = generate_tokenizer(eng_corpus, SRC_VOCAB_SIZE, \"en\")\n",
    "# ko_test_tokenizer = generate_tokenizer(kor_corpus, TGT_VOCAB_SIZE, \"ko\")\n",
    "# ko_test_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bdff11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_corpus_test:   cristiano will begin his convalescence under the direction of club medical staff and an estimation of his return to full fitness will be possible following review by the specialist in one month . \n",
      "kor_corpus_test:  맨유는 또 호날두는 의료진의 보호 아래 회복기를 가질 것 이라며 전문가들은 호날두가 한달 후에 건강한 모습으로 돌아온다고 말했다 고 전했다 . \n"
     ]
    }
   ],
   "source": [
    "print('eng_corpus_test: ', eng_corpus_test[100])\n",
    "print('kor_corpus_test: ', kor_corpus_test[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8368623c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 611\n",
      "문장의 평균 길이: 135\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eng_corpus 통계\n",
    "\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "\n",
    "for sen in eng_corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(eng_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "#총 max_len의 배열을 만든 후, raw 문장을 돌면서 각 문장별 길이를 sentence_length의 len(sen) 인덱스마다  계속 더해가면서 counting\n",
    "for sen in eng_corpus:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3dec517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 331\n",
      "문장의 평균 길이: 64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kor_corpus 통계\n",
    "\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "\n",
    "for sen in kor_corpus:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(kor_corpus))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "#총 max_len의 배열을 만든 후, raw 문장을 돌면서 각 문장별 길이를 sentence_length의 len(sen) 인덱스마다  계속 더해가면서 counting\n",
    "for sen in kor_corpus:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "006250f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_tokens = np.mean(num_tokens) + 2.5 * np.std(num_tokens)\n",
    "# maxlen = int(max_tokens)\n",
    "# print('pad_sequences maxlen : ', maxlen)\n",
    "# print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)*100}%가 maxlen 설정값 이내에 포함됩니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de6da326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f501a7f9352d4d0687a9b2336d4db6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78968 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저를 활용해 토큰의 길이가 50 이하인 데이터를 선별하여 src_corpus 와 tgt_corpus 를 각각 구축하고, 텐서 enc_train 과 dec_train 으로 변환\n",
    "src_corpus = [] #영어\n",
    "tgt_corpus = [] #한글\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 xxx 이하인 문장만 남깁니다. \n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "    src = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
    "    tgt = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
    "    \n",
    "    if len(src) <= 120 and len(tgt) <= 100: \n",
    "        src_corpus.append(src)\n",
    "        tgt_corpus.append(tgt)\n",
    "\n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff7de7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 훈련 데이터와 검증 데이터로 분리하기\n",
    "# enc_train, enc_val, dec_train, dec_val = train_test_split(enc_data, dec_data, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c551036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78967, 116)\n",
      "(78967, 95)\n"
     ]
    }
   ],
   "source": [
    "# enc, dec 의 seq_length는 달라도 상관없음.\n",
    "print(enc_train.shape)\n",
    "print(dec_train.shape)\n",
    "# print(enc_val.shape)\n",
    "# print(dec_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b44af4d",
   "metadata": {},
   "source": [
    "# Step 3. 모델설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c66392b",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff5c5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos - 단어가 위치한 Time-step(각각의 토큰의 위치정보값이며 정수값을 의미)\n",
    "# d_model - 모델의 Embedding 차원 수\n",
    "# i - Encoding차원의 index\n",
    "\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i)/d_model)  # np.power(a,b) > a^b(제곱)\n",
    "    \n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "    \n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    \n",
    "    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    \n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122e57a",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60206f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)  # Linear Layer\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        \n",
    "        # Scaled QK 값 구하기\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9)\n",
    "        \n",
    "        # 1. Attention Weights 값 구하기 -> attentions\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        # 2. Attention 값을 V에 곱하기 -> out\n",
    "        out = tf.matmul(attentions, V)\n",
    "        return out, attentions\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Embedding된 입력을 head의 수로 분할하는 함수\n",
    "        \n",
    "        x: [ batch x length x emb ]\n",
    "        return: [ batch x length x heads x self.depth ]\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "        return split_x\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        분할된 Embedding을 하나로 결합하는 함수\n",
    "        \n",
    "        x: [ batch x length x heads x self.depth ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "        return combined_x\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        \"\"\"\n",
    "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
    "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
    "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
    "                 -> out, attention_weights\n",
    "        Step 4: Combine Heads(out) -> out\n",
    "        Step 5: Linear_out(out) -> out\n",
    "        \"\"\"\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d26e9",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45aabf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785bf1b",
   "metadata": {},
   "source": [
    "## Encoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9031519",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "        \n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        # Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual*1 \n",
    "        \n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual*1 \n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0306f2",
   "metadata": {},
   "source": [
    "## Decoder 레이어 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc11c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        #out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        #out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc1770",
   "metadata": {},
   "source": [
    "## Encoder와 Decoder 클래스를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45dbc8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "            \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "237de8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        \n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caf10a8",
   "metadata": {},
   "source": [
    "## Transformer 완성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24eca66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, src_vocab_size, tgt_vocab_size,\n",
    "                 pos_len, dropout=0.2, shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        # 1. Embedding Layer 정의\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # 2. Positional Encoding 정의\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        # 6. Dropout 정의\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "        # 3. Encoder / Decoder 정의\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        \n",
    "        # 4. Output Linear 정의\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "        \n",
    "        # 5. Shared Weights\n",
    "        self.shared = shared\n",
    "        \n",
    "        if shared:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "        \n",
    "        \n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        입력된 정수 배열을 Embedding + Pos Encoding\n",
    "        + Shared일 경우 Scaling 작업 포함\n",
    "\n",
    "        x: [ batch x length ]\n",
    "        return: [ batch x length x emb ]\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "        \n",
    "        if self.shared:\n",
    "            out *= tf.math.sqrt(self.d_model)\n",
    "        \n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        # Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "        \n",
    "        # Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "\n",
    "        # Step 3: Decoder(dec_in, enc_out, mask) -> dec_out, dec_attns, dec_enc_attns\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
    "        \n",
    "        # Step 4: Out Linear(dec_out) -> logits\n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fb188",
   "metadata": {},
   "source": [
    "## Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85de1255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention을 할 때에 <PAD> 토큰에도 Attention을 주는 것을 방지해 주는 역할\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(src_len, tgt_len):\n",
    "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
    "    return tf.cast(mask, tf.float32)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_mask = generate_padding_mask(tgt)\n",
    "\n",
    "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
    "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
    "\n",
    "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
    "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86c056a",
   "metadata": {},
   "source": [
    "# Step 4. 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bafa861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=6,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.2,\n",
    "    shared=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6c054",
   "metadata": {},
   "source": [
    "##  Learning Rate Scheduler를 선언하고, 이를 포함하는 Adam Optimizer를 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c525f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2028630",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6f375",
   "metadata": {},
   "source": [
    "## Loss 함수를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67a4c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72c382",
   "metadata": {},
   "source": [
    "## train_step 함수를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ad4e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c75afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer = optimizer , transformer = transformer)\n",
    "manager = tf.train.CheckpointManager(ckpt, './tf_ckpts_gd12_r1',max_to_keep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6bbce",
   "metadata": {},
   "source": [
    "## Attention 시각화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3da0af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e062526",
   "metadata": {},
   "source": [
    "## 번역생성함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88f5dbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 함수\n",
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
    "\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens], maxlen=enc_train.shape[-1], padding='post')\n",
    "\n",
    "    ids = []\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
    "    for i in range(dec_train.shape[-1]):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(_input, output)\n",
    "        \n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(_input, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        \n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
    "        if tgt_tokenizer.eos_id() == predicted_id:\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dedc5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d3b34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"How was your day? I was the best.\",\n",
    "    \"Take your time, please.\",\n",
    "    \"I’m about to leave. Please hold for a moment.\",\n",
    "    \"Have you heard of it?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af028dad",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "650ac423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def train_and_checkpoint(transformer, manager, EPOCHS):\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    \n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "\n",
    "        idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "        random.shuffle(idx_list)\n",
    "        t = tqdm(idx_list)\n",
    "\n",
    "        for (batch, idx) in enumerate(t):\n",
    "            batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                                                         dec_train[idx:idx+BATCH_SIZE],\n",
    "                                                                         transformer,\n",
    "                                                                         optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "      \n",
    "      \n",
    "        # 매 Epoch 마다 제시된 예문에 대한 번역 생성\n",
    "        for example in examples:\n",
    "            translate(example, transformer, en_tokenizer, ko_tokenizer)\n",
    "            \n",
    " \n",
    "        if int(ckpt.step) % 2 == 0:\n",
    "            save_path = manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d519d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba7d4bf5e1747f4a553118bdd7077b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 그는 또 다른 사람들의 이야기를 통해 우리가 될 것이다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 그는 또 다른 사람들은 자신의 이야기를 위한 것이다 .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 그는 또 다른 사람들은 더 많은 사람들이 더 많은 것을 알고 있다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 그는 우리가 우리가 ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0526dc9db73d46ed8ecaf4f44ebeb8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 그는 또 얼마나 잘 알고 있다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 다음 보기\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 그는 또 나는 자신의 집을 찾는 것을 목격했다 며 나는 그가 자신의 집을 받을 수 있다 고 말했다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 여러분은 동영상이 언제 ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c3fd4cb2eb448398a004d750d1baf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 그는 이틀 동안 얼마나 많은 순간을 날랐다 며 그녀가 어떻게 기억됐는지는 말하지 않았다 고 말했다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 당신에게 당신의 시간을 보내야 할 것\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 그는 내가 내가 난 후 다시한번 다시한번 기도할 것 이라며 난 후 다시한번 내가를 할 것 이라며 난 순간 이라고 말했다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 노래는 소리 !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334369d1941f418b9aa23e1b863e6816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 얼마나 날지는 기억이 날이었다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 당신의 시간을 보내는\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 나는 나는 나는 나는 그들이 원하는 사람들을 위해 줄을 서서히 걸었다 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation:  ⁇  소리는 소리 ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5858aba373c24d8bb5d8a4643c6e254d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 그는 어떻게 대처할 지에 대해 궁금해 하고 있는지 모르겠다 고 말했다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 당신은 당신을 위해 구직을 위해 벌금을 던지고 있다 .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 나는 그를 위해 약 만원 에는 비행기편을 찾아라 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 당신은 무슨 말을 하고 있었을까 ?\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c78f3045830438cb487689960a9876c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How was your day? I was the best.\n",
      "Predicted translation: 어떻게 이 날 가장 좋은 경험을 어떻게 했다 .\n",
      "Input: Take your time, please.\n",
      "Predicted translation: 그러나 당신에게 시간을 잊지 말 것을 요청한다 .\n",
      "Input: I’m about to leave. Please hold for a moment.\n",
      "Predicted translation: 나는 그라 .\n",
      "Input: Have you heard of it?\n",
      "Predicted translation: 당신은 당신의 정신적인가 ?\n"
     ]
    }
   ],
   "source": [
    "train_and_checkpoint(transformer, manager, EPOCHS )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84a4c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: by the nanjing no . intermediate people s court for usage of a dangerous substance , the official xinhua news agency reported . \n",
      "Predicted translation: 롬니는 이런 상황을 믿을 수 없다 고 전했다 .\n",
      "\n",
      "Input: most of the milk sent by regular mail and addressed to fischer boel had spoiled in transit , and some of the cartons had burst open , he said . \n",
      "Predicted translation: 헤틀랜 대변인은 또 게디가 전에 의사가 아들을 찾는데 동의했다고 전했다 .\n",
      "\n",
      "Input: south korean experts claim that japan is exploiting its reinforced alliance with the united states as a chance to foster its military buildup . \n",
      "Predicted translation: 한국의 동맹은 일본이 일본과의 동맹 관계를 강화시키고 영국에 미국에 재방할 것이라고 주장했는데 , 일본은 이를 받아들였다고 주장하여 우리의 동맹을 반박했다 .\n",
      "\n",
      "Input: falling oil prices also helped spark a strong stock market rally . \n",
      "Predicted translation: 증시 폭락으로 증시도 상승세를 보였습니다 .\n",
      "\n",
      "Input: accompanying obama is sen . jack reed of rhode island , a leading democrat on the senate armed services committee , and sen . chuck hagel of nebraska , a republican member of the foreign relations committee and an outspoken critic of the iraq war . \n",
      "Predicted translation: 이라크 내 에서의 상원 의원은 오바마 상원 의원의 이라크 전 상원의원인 버라브러미온 상원 의원 투표에서 이라크의 이라크 전과 상원 의원에 대한 지지를 받고 있다 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in eng_corpus_test[:5]:\n",
    "    translate(example, transformer, en_tokenizer, ko_tokenizer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f0de6",
   "metadata": {},
   "source": [
    "# Step 5. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1c939c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, en_tokenizer, ko_tokenizer, verbose=True):\n",
    "    src_tokens = en_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = ko_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > 120): return None\n",
    "    if (len(tgt_tokens) > 100): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_sentence, model, en_tokenizer, ko_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e281591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: south korean experts claim that japan is exploiting its reinforced alliance with the united states as a chance to foster its military buildup . \n",
      "Predicted translation: 한국의 동맹은 일본이 일본과의 동맹 관계를 강화시키고 영국에 미국에 재방할 것이라고 주장했는데 , 일본은 이를 받아들였다고 주장하여 우리의 동맹을 반박했다 .\n",
      "Source Sentence:  south korean experts claim that japan is exploiting its reinforced alliance with the united states as a chance to foster its military buildup . \n",
      "Model Prediction:  ['한국의', '동맹은', '일본이', '일본과의', '동맹', '관계를', '강화시키고', '영국에', '미국에', '재방할', '것이라고', '주장했는데', ',', '일본은', '이를', '받아들였다고', '주장하여', '우리의', '동맹을', '반박했다', '.']\n",
      "Real:  ['한국의', '외교', '전문가들은', '일본이', '강화된', '미국과의', '동맹을', '군사력', '증강에', '이용하고', '있다고', '주장한다', '.']\n",
      "Score: 0.012918\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01291802583371725"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single test\n",
    "test_idx = 2\n",
    "\n",
    "eval_bleu_single(transformer, \n",
    "                 eng_corpus_test[test_idx], \n",
    "                 kor_corpus_test[test_idx], \n",
    "                 en_tokenizer, \n",
    "                 ko_tokenizer, \n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c7c2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, en_tokenizer, ko_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], en_tokenizer, ko_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b4bc10f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02992f58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd7638c27844b68aabf0d1f1ef4d584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: by the nanjing no . intermediate people s court for usage of a dangerous substance , the official xinhua news agency reported . \n",
      "Predicted translation: 롬니는 이런 상황을 믿을 수 없다 고 전했다 .\n",
      "Input: most of the milk sent by regular mail and addressed to fischer boel had spoiled in transit , and some of the cartons had burst open , he said . \n",
      "Predicted translation: 헤틀랜 대변인은 또 게디가 전에 의사가 아들을 찾는데 동의했다고 전했다 .\n",
      "Input: south korean experts claim that japan is exploiting its reinforced alliance with the united states as a chance to foster its military buildup . \n",
      "Predicted translation: 한국의 동맹은 일본이 일본과의 동맹 관계를 강화시키고 영국에 미국에 재방할 것이라고 주장했는데 , 일본은 이를 받아들였다고 주장하여 우리의 동맹을 반박했다 .\n",
      "Input: falling oil prices also helped spark a strong stock market rally . \n",
      "Predicted translation: 증시 폭락으로 증시도 상승세를 보였습니다 .\n",
      "Input: accompanying obama is sen . jack reed of rhode island , a leading democrat on the senate armed services committee , and sen . chuck hagel of nebraska , a republican member of the foreign relations committee and an outspoken critic of the iraq war . \n",
      "Predicted translation: 이라크 내 에서의 상원 의원은 오바마 상원 의원의 이라크 전 상원의원인 버라브러미온 상원 의원 투표에서 이라크의 이라크 전과 상원 의원에 대한 지지를 받고 있다 .\n",
      "Input: more than half of all yemeni girls are married off before the age of , according to oxfam international , a nonprofit group that fights global poverty and injustice . \n",
      "Predicted translation: 국제 사회 전문기구인 국제재연맹 international union for international consumer electronics , 전세계 아이들의 딸들과 비교해 .\n",
      "Input:  cnn police in western china shot and killed five people in a group that was planning a holy war against han chinese , the largest ethnic group in china , police told china s xinhua news agency wednesday . \n",
      "Predicted translation: 중국 베이징에서 티베트 소요사태 도중 총에 맞았다고 일 현지시간 중국 경찰과 서방 언론이 티베트의 정신적인 지도자인 티베트의 정신적인 대인택로 사망했다고 경찰이 밝혔다 .\n",
      "Input: but there are still several hurdles that must be overcome before the agreement is implemented . \n",
      "Predicted translation: 그러나 합의문 이행이 몇기 전까지는 계속 될 전망이다 .\n",
      "Input: asked if what she went through was torture , she nods quietly . \n",
      "Predicted translation: 그는 고문이 고문했을 때 차량에는 유감을 호소했다 .\n",
      "Input: the . kilometer . mile race started in ehrwald , austria . \n",
      "Predicted translation: km의 산유권은 일 오스트리아 수도에서 열릴 예정이다 .\n",
      "Input: choi added that he is worried about the former president as he will have to see many of his former aides get summoned by prosecutors . \n",
      "Predicted translation: 최기 전 대통령은 검찰 총장의 결정에 대해 검찰 총장을 검찰 측에서 검찰 총장의 결정에 우려를 나타냈다 .\n",
      "Input: have come too late for many asian exporters who will still face delays of up to weeks to get their products unloaded . \n",
      "Predicted translation: 아시아나에 거주하는 아시아의 주인들은 자신들의 컴퓨터 게임을 생산할 목적으로 여러 주어질 예정이다 .\n",
      "Input: but the federal office of thrift supervision moved to seize its assets friday afternoon following a two week run on the bank , during which customers withdrew more than . billion . \n",
      "Predicted translation: 그러나 연방순회계의 거부권을 행사한 래인 애슬로페는 일 현지시간 자산을 동결하는 작업에 착수했다 .\n",
      "Input: the gap is expected to widen this year , according to the korea tourism organization kto . \n",
      "Predicted translation: 이 같은 통계는 년 한국 국민수 한국을 방문하는 것으로 나타났다 .\n",
      "Input: freddie fre , fortune shares tumbled , leaving them down in july . \n",
      "Predicted translation: 프레드릭 이익은 지난 월 일 , 주택차부품인 코니맨에 대해 다시한번 자신의 생각을 바꿀 수 없었다 .\n",
      "Input: mccain met with uribe after arriving in colombia on the first leg of a three day trip to latin america \n",
      "Predicted translation: 지난 일 콜롬비아의 모푸토가 방문 중 가장 많은 적은 모습을 드러냈다 .\n",
      "Input: music and video quality were largely unchanged , but we didn t have many complaints in that department to begin with . \n",
      "Predicted translation: 음악과 동영상 부문은 모두 가장 큰 관심사로 이어진다 .\n",
      "Input: barcelona you re pretty safe walking anywhere in the main tourist areas , but be wary of strangers who are a little too friendly , says a police spokeswoman who was not authorized to give her name . \n",
      "Predicted translation: 바르셀로나는 냉전이고 뛰어나는 수많은 관광객들을 경찰서의 습격을 받아 관광객들이 너무 많다 며 그들이 원하는 것은 너무 많다 고 말했다 .\n",
      "Input: thirty five percent say they re still on dial up because broadband prices are too high , while another percent say nothing would persuade them to upgrade . the remainder have other reasons or do not know . \n",
      "Predicted translation: 또 다른 게임 중 가난데도 , 왜 다시 불을 지킬 수 있다고 해서는 안도어 가격이 비떨지도 않았습니다고 해서 비 씨티 씨티은행의 가 밝혔습니다 .\n",
      "Input: his open defiance of the republican establishment has helped to bolster his image as a maverick lawmaker willing to shun party loyalty and obedience for personal principle . \n",
      "Predicted translation: 공화당 의원들은 공화당 의원들에게 이라크 지도자인 패할 수 있는 공화당 의원들의 지지를 촉구하는 법안을 제출했다 .\n",
      "Input: saudi arabia is the largest producer in the oil producers cartel , with an allocation of . million barrels a day , but has announced it is hiking production to . million . \n",
      "Predicted translation: 사우디 최대 석유 생산량은 만톤에 이르고 있지만 석유 생산량은 만 배럴로 감축된다 .\n",
      "Input: as the money comes in for obama , his campaign is investing in states that have traditionally been republican strongholds . \n",
      "Predicted translation: 오바마의 공화당 대선주자인 존 매케인은 오바마가 선거운동 중이었다면 그들은 그가 전통적이라고 판단한다 .\n",
      "Input: the tiger was too shiny , they said . and no matter where it was snapped among the trees , its position never changed . \n",
      "Predicted translation: 이 호랑이는 매우 나무 위가  ⁇ 어져 있었다 며 나무에 따른 영향을 끼지않는데 영향을 끼쳤지 못했다 .\n",
      "Input: high energy costs will remain a drag on the u . s . economy for the rest of the year , bernanke told the senate banking committee tuesday . \n",
      "Predicted translation: 고유가 , 고유가 , 군락 , 미 상원 외교위원회 및 경기락에 대한 전반적인 노력을 다시 수행하게 될 것\n",
      "Input: what is paulson s bailout strategy ? \n",
      "Predicted translation: 자동차업계에 대한 전반적인 내용은 무엇인가 ?\n",
      "Input:  cnn two of five feet that have washed up on the shores of british columbia are from the same person , but authorities said tuesday they are a long way from solving the mystery of where they came from . \n",
      "Predicted translation: 영국 bbc에서 일 오전에는 약 마리의 시신이 수습됐다고 전했다 .\n",
      "Input: president george w . bush and german chancellor angela merkel pledged tuesday to keep working together on common problems , but progress appeared slow on reaching a consensus on climate change as the group of eight major economies tackled that and other knotty global issues . \n",
      "Predicted translation: 원문기사보기 조지 w . 부시 대통령과 앙겔라 메르켈 독일 총리가 국제적 경제 문제에 개입하지 않으면 국제적 경제문제가 주요무역에 맞서 국가의 주요 의제가 될 것으로 보인다 .\n",
      "Input: she received an cm long four inch cut to her cheek which required stitches . \n",
      "Predicted translation: 그는 약 . . . . . . . . m 깊이의 얼굴을 확인한다 .\n",
      "Input: wal mart spokesman lorenzo lopez said the retailer has instructed stores to remove the books from shelves and discontinue sales . \n",
      "Predicted translation: 한편 그런가 하면 이 같은 일이 판매될 수 있도록 압력을 가하기 위해 발디즈에게 판매를 지시했다고 대변인이 밝혔다 .\n",
      "Input: the swiss daily tribune de geneve reported that two of gadhafi s domestic servants claimed gadhafi and his wife repeatedly beat them at the president wilson hotel , which is just next door to the united nations human rights office . \n",
      "Predicted translation: 피찬 일간지 피찬 일간지 칼라 cad paleonslide 은 일 현지시간 일간지와의 인터뷰에서 자신이 직접 부인하는 것을 본다고 보도했다 .\n",
      "Input: among those to brave the tv cameras at the ronald reagan presidential library in simi valley , california , are rudi giuliani , john mccain and mitt romney . \n",
      "Predicted translation: 볼 때는 우리는 캘리포니아의 국기를 들르며 , 캘리포니아의 볼이리의 상징인 존 f , 캘리포니아 , 캘리포니아의 선거 이후도 모두 위에 있다 .\n",
      "Input: the former untouchables of alwar were invited to new york to illustrate that point and also be honored . \n",
      "Predicted translation: 안와르 전 부총리는 미국과 아랍 국적상은 없으며 안와르는 이 같은 업적을 보였다 .\n",
      "Input: if you re comfortable standing around in a tank top , you re probably close enough . \n",
      "Predicted translation: 만약 당신이 내 탱크를 갖추면 당신은 더욱 꼬드슨 를 끌 뿐이다 .\n",
      "Input: almost half of juniors surveyed , who will get their first voting rights in the presidential election , said in a recent poll that south korea should side with north korea if washington attacks nuclear facilities in the north without seoul s consent . \n",
      "Predicted translation: 미국 정부가 실시한 예비 선거 및 북한의 동북아 지역의 투표율이 이번 에 실시된다 .\n",
      "Input: in the town of alwar in the northern state of rajasthan , there are about so called untouchable women working in this profession . \n",
      "Predicted translation: 안와르 전 부총리는 이날 잔해에서 도로가 발생하는 사고가 발생했다 며 이 지역은 인도 전역에는 만 명이다 고 밝혔다 .\n",
      "Input: british prime minister gordon brown on wednesday said mugabe has blood on his hands after the violence leading up to last week s election and should step down . \n",
      "Predicted translation: 고든 브라운 영국 총리는 일 현지시간 지난주 키바키 대통령과의 퇴진을 요구하는 시위를 벌일 것이라고 밝혔다 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: at the white house on wednesday , bush noted that there were only legislative days left in the fiscal year and said congress would need to pass a spending bill every other day to get their fundamental job done . \n",
      "Predicted translation: 백악관은 일 현지시간 부시 미 대통령의 거부권 행사가 별다른 영향 없이 법안 통과가 이뤄질때까지 추가적인 행사가 필요하진 않을 것이라고 밝혔다 .\n",
      "Input: the problem here is obvious \n",
      "Predicted translation: 문제 해결방안에 문제는 중요한 문제 라고 말했다 .\n",
      "Input: the center says mudslides buried two houses that killed seven people in kaohsiung , southern taiwan . \n",
      "Predicted translation: 원문 독해설명 인쇄\n",
      "Input: medical , forensic and ballistic tests conducted in several u . s . , russian and austrian laboratories identified bone and tooth fragments unearthed last summer as belonging to two missing children of czar nicholas ii . \n",
      "Predicted translation: 그는 영국 런던 중심가에서도 dna 테스트와 사망자 중 많은 수가 모두 유사한 이유로 러시아인 해군과 러시아인 올 봄날개로하였다 .\n",
      "Input:  i have this morning asked the treasury to work with the financial action task force to track the wealth and the assets that are owned by members of the mugabe regime which we know are held in different continents including asia , africa , and europe , he said , so that we are in a position to take tougher action against them at a later date . \n",
      "Predicted translation: 그는 일 오전 현지시간 미국 경제 협력체 자산 관리를 위해 유럽 연합과의 노력이 유럽 연합에 협력할 것을 요청했으며 이 같은 결정을 위해 유럽 연합과의 노력이 필요한무역협력을 제공해 줄 수 있도록 노력할 것 이라고 말했다 .\n",
      "Input: to do less would merely satisfy our need for revenge but would doom our children to a new dark age . \n",
      "Predicted translation: 그는 아이가 자아선에 가는 것이지만 성이라는 것이지만 나는 성의 감적이라는 것이지만 필요할 것이다 .\n",
      "Input: but the men her captors handed her over to turned out to be colombian troops who had tricked the revolutionary armed forces of colombia commanders have held hundreds of hostages in the jungle for years . \n",
      "Predicted translation: 그러나 콜롬비아 무장혁명군 farc 은 그가 인질로 잡혀있는 인질을 데리고 가둔 군인 명을 납치했고 나머지 명은 년 전 콜롬비아 무장혁명군 farc 에 보낸 것으로 알려졌다 .\n",
      "Input: regional officials said he died in a shootout when marines swooped on his hideout in the island of tawi tawi . \n",
      "Predicted translation: 현재 미국 관리들은 전라 사망사고가 발생한 년 전에도 최악의 사망 사건을 조사하고 있다 .\n",
      "Input: an infusion of billions of dollars into the money supply by the fed buoyed financial stocks , which rescued the market from another big slide . \n",
      "Predicted translation: 현금이  ⁇ 기 위한 자금 중 일부를 운영하고 있는 가운데 이는 미국의 금융구제재개로 된 다른 공급원들이 억 달러를 잃고 있다 .\n",
      "Input: the company said in a statement that an adverse event report does not mean that a causal relationship between an event and vaccination has been established \n",
      "Predicted translation: 이 회사는 성명에서 어떤 면에서는 이미 사랑과 싸우는 관계가 널리 퍼지지 않은 것에 대해 깊은 유감을 표시하고 있다 고 말했다 .\n",
      "Input: no smoking , no crossing over guardrails , no use of umbrellas or standing up for a long period of time in the seating area , and no flash photography . \n",
      "Predicted translation: 그는 흡연을 하지 않는 층 건물 , 빈도 , 흡연을 강요하거나 , 담배를 피웠던지 , 흡연을 위해 필요한 시간을 들여다 볼 수 있다 고 지적했다 .\n",
      "Input: the destruction of the highly visible symbol of north korea s long secret nuclear program came just a day after the country released details of its program . \n",
      "Predicted translation: the highest level militarys 북한 지도자는 핵 프로그램 검증 문제를 해결하기 위한 노력의 일환으로 , 오늘 아침의 개성에 대한 자세한 내용을 공개하며 이번 회담은 원래의 자세한 내용을 보여준다 .\n",
      "Input:  year old brown was charged in march by los angeles county prosecutors with felony assault and making criminal threats . \n",
      "Predicted translation: 브라운 총리는 지난 월 미성년자들과 건의 형사 재판을 받은 혐의로 기소를 받은 바 있다 .\n",
      "Input: two pilots , two medical officers and three soldiers died , the official said . \n",
      "Predicted translation: 관계자는 병사 명이 숨졌으며 그중 명이 중태라고 전했다 .\n",
      "Input: bbc reports from communist china say a man has been shot dead after taking australian tourists hostage . \n",
      "Predicted translation: bbc 방송은 남성이 실종 기간 중 호주 관광객 명을 사살하고 있다는 사실을 호주 관광객에게 총상을 발급했습니다 .\n",
      "Input: right now , the dow is up points . \n",
      "Predicted translation: 현재 다우지수는 포인트 올랐습니다 .\n",
      "Input: treasury secretary alistair darling said the postponement of the pence u . s . cent per liter increase in fuel tax would help motorists get through what is a difficult time for everyone . \n",
      "Predicted translation: 알리스타 달링 재무장관은 이날 오후 가진 인터뷰에서 휘발유가 미국과 영국간의 통화로 조정을 위해 세금을 붙일 경우 동시 에 교체될 것 이라고 말했다 .\n",
      "Input: but assistant secretary of state dan fried said the united states has taken to heart polish concerns over more u . s . cooperation with russia and nato on the missile defense shield . \n",
      "Predicted translation: 이에앞서 라이스 국무장관이 러시아의 미사일 시험 발사는 러시아의 민감한 정보를 갖고 러시아의 군사력을 향상시킬 목적으로 미국과 러시아의 동맹 관계가 옳다고 생각하지 않는다고 밝혔다 .\n",
      "Input: most of the north s ships loaded with oil or cement are moving from the northeast coast to the northwest coast via waters surrounding cheju island , according to the korea coast guard . north korean ships are still passing through our waters , a coast guard official said . \n",
      "Predicted translation: 북한 화물선 선박이 동해 부근 해안을 따라 연안을 해안으로 접근했다 .\n",
      "Input: the latest injuries bring to the number of runners who have been hurt in the first three days of the running . \n",
      "Predicted translation: 최근의 사망자는 마지막으로 부상을 당했습니다 .\n",
      "Input: she says that people were aboard the ride when it collapsed and that the national board of accident investigation will start an inquiry . \n",
      "Predicted translation: 그녀는 자신이 알고 있는 사고의 정확한 사고 경위 조사를 받았을 것이라고 말했습니다 .\n",
      "Input: then in december , rodriguez signed a record million , year contract with new york , a deal that allows him to make up to million if he reaches milestone achievements . \n",
      "Predicted translation: 이전 년에는 만달러 약 억원 이상의 투표율을 기록 , 만달러를 넘게  ⁇ 타르로 거래를 마쳤다 .\n",
      "Input: after that , the dialogue was delayed for five years over cross strait tensions . \n",
      "Predicted translation: 회담 후 , 한번에 걸쳐 대화가 지연된 후 , 그는 대화가 지연됐다 .\n",
      "Input: the singers stirred up controversy that year with an open mouth kiss at the mtv video music awards . \n",
      "Predicted translation: 가수상은 음악 트랙에서의 성과를 상한 개가 논란이 일었을 ?\n",
      "Input: the dead include a norwegian journalist and a us citizen as well as a number of security guards . \n",
      "Predicted translation: 노르웨이 외무장관 , 노르웨이의 노르웨이 정부 관계자는 노르웨이의 안전을 위해 경호를 강화했다 .\n",
      "Input:  we heard an explosion , then the dust and glass hit our faces , a resident near the scene of the blast said . after that we saw that people were dead and lying everywhere . \n",
      "Predicted translation: 우리는 폭발 현장을 목격한 한 주민이 현장에서 구조를 듣고 현장이 현장을 향해 총격을 가 ⁇ 다 며 갑자기 폭발로 폭발해도 대피했지만 사상자 발생 후 몇 명이 숨졌다 고 말했다 .\n",
      "Input:  i was very aggressive on my forehand and backhand and had a lot of power . but i was surprised to win in straight sets and i m happy to be in the semifinals . \n",
      "Predicted translation: 그는 오늘은 매우 힘이 돼 기쁘다 며 그러나 오늘 아침 자신의 승리를 위해 모든 것을 마음을 먹고 승리를 가져다 주는 것이 분명하다 고 덧붙였다 .\n",
      "Input: airtran employs about , people , including , pilots and , flight attendants . \n",
      "Predicted translation: 이 조종사는 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 , 조종사 등이다 .\n",
      "Input:  . the birds alfred hitchcock , \n",
      "Predicted translation: . 테러로 사고 현장에 현다\n",
      "Input: the united states has also agreed to help poland modernize its military , which it requested as a condition of its support for housing the missile defense system . \n",
      "Predicted translation: 미국은 또한 군사 지휘 시스템을 지지하기로 합의했다 .\n",
      "Input: pyongyang agreed to shut the yongbyon plant under a deal reached in february . \n",
      "Predicted translation: 평양은 월 합의를 월 이내에 김은 영변 원자로를 폐쇄했다 .\n",
      "Input: defendants don t have to miss a day of work to appear before the judge . \n",
      "Predicted translation: 법정에 출전하기 위해 하루 전에 판사 앞 에서 판사 앞 에서 하루 전에는 판사 앞 에서 하루 전에 판사 앞둔 판사 앞 에서 하루 전에 판사 앞 에서 하루가 남았다 .\n",
      "Input: julie , a housewife in los angeles , said she thought about buying gas masks for herself , her husband and her two young sons , but then decided it would be futile to try to change one s fate under such circumstances . \n",
      "Predicted translation: 줄리 거스키 사장은 차 결과에 대해 남편인 가오로는 남편의 가 너무 많지만 차를 몰고 가라가는 것을 본 것이지만 아들은 결국 그를 위해 계속해서 결혼할 것이라고 말했다 .\n",
      "Input: it said a cm wave hit nemuro port in hokkaido island . \n",
      "Predicted translation: 이는 비록 번째 총알이 맞은 해 연결된 지건이 될 수 있다 .\n",
      "Input: it encourages people to discard age old practices of urinating and defecating in the open , leading to diseases . and the waste product goes into research to test their effectiveness as fertilizers . \n",
      "Predicted translation: 그것은 고래를 살 수 있는 기회를 낳을 수 있는 기회를 찾는데 도움이 될 것이다 .\n",
      "Input: the united states pushed the resolution last week after mugabe ignored the security council s appeal to postpone a presidential runoff election . \n",
      "Predicted translation: 한편 지난주 미국은 러시아의 총선을 실시할 예정이다 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: democrats in congress have blocked the deal , citing the intimidation and killing of colombian labor activists by right wing paramilitary troops . \n",
      "Predicted translation: 민주당원들은 , 의회는 수지방을 불안정하게 된 콜롬비아의 군 복무와 반여자였다 .\n",
      "Input: baghdad , iraq cnn presumptive u . s . democratic presidential candidate barack obama arrived in iraq on monday for talks with iraqi officials and american military commanders in a year old war he has pledged to end , a u . s . embassy spokesman said . \n",
      "Predicted translation: 이라크에서 활동하고 있는 일이 일 현지시간 이라크 주둔 미군 사령관과 미국간의 긴장이 고조되는 가운데 미군은 일 현지시간 이라크 주둔 미군의 이라크 주둔 기간을 위해 이라크 주둔 미군을 방문 , 이라크 전쟁으로 돌아가자 , 이라크 관리들이 밝혔다 .\n",
      "Input:  cnn an year old man was forced to undergo emergency surgery to have his leg amputated following a hospital blunder in which the broken limb was encased too tightly in a plaster cast . \n",
      "Predicted translation: 총상을 입은 살의 소년은 병원으로 이송되어 치료를 받은 뒤 나머지 팔은 비상 활반구에 빨간 불이 붙은 사건이 일어났다 .\n",
      "Input: the man was a member of the colombian military intelligence team involved in the daring rescue , uribe said in an address carried on national tv and radio . \n",
      "Predicted translation: 콜롬비아 정부군이 지난 년 콜롬비아무장혁명군 farc 에 피랍된 다음날 fbi 본부를 둔 이 남자와 라디오를 포함 , 명의 주민을 사살한 것으로 알려졌다 .\n",
      "Input: advice from a lawyer was that the winning bidder would not be entitled to anything but scott s soul and would not be able to own or control him in any way , he said . \n",
      "Predicted translation: 스커드의 변호사가 스패를 얻는 것이지만 그가 직업적이나 통제력을 드러내지 않을 경우 부인이나 자녀를 위해 고안된 것이 아니라는 점을 강조했다 .\n",
      "Input: in a fundraising e mail to supporters , obama campaign manager david plouffe acknowledged the deficit , saying mccain and the rnc together still have a huge cash advantage , and we need your help to close the gap . \n",
      "Predicted translation: 클린턴 지지자 중 상당수는 현금 모금활동을 하며 오바마는 선거를 지지하는 유권자들에게 필요한 자금을 투입했음을 시인했다 .\n",
      "Input: the school said the panel will initially look into data used by hwang images and dna fingerprints of stem cells in response to suspicions from some scientists that they were fabricated . \n",
      "Predicted translation: 서울대 조사위원회는 황박사가 지문과 관련 , 인간의 접촉을 통해 줄기세포 연구를 통해 여러 차례에 걸쳐서 정밀 검사를 받을 것이라고 밝혔다 .\n",
      "Input: once known for a deceptive delivery and a devastating forkball , the year old nomo was released by the kansas city royals in late april . \n",
      "Predicted translation: 년이 된 , 지금 까지 , 막을 회복했고 , 년 월 일로즈볼에서 정치적인 전환을 포기했었다 .\n",
      "Input: before your next party , go ahead and consult the latest edition of merriam webster s collegiate dictionary , which now includes edamame immature green soybeans , pescatarian a vegetarian who eats fish and about other newly added words that have taken root in the american lexicon . \n",
      "Predicted translation: 자 회담이 들어 있는 시카고의 퍼펙과 결혼한 뒤를 포함해  ⁇ 의 사인을 풀고 풀이 들어 가는 인기를 얻었다 .\n",
      "Input: they also plan to meet with troops from their home states and u . s . civilians working in iraq , cucciniello said . \n",
      "Predicted translation: 이들은 또한 이라크의 미군을 휴가를 준지로 협의했으며 일 밤 에 자이리지와도 문제를 가지기 위해 노력했다 고 말했다 .\n",
      "Input: berlin , germany cnn a man raced into berlin s madame tussauds wax museum saturday and ripped the head off a waxwork of adolf hitler , police said . \n",
      "Predicted translation: 이곳에서 발행된 독일 남성이 일 현지시간 다량의 한 남자가 박물관을 상대로 전시 전시를 벌여 그를 체포했다고 일 현지시간 유엔 관계자가 밝혔다 .\n",
      "Input:  we are trying to ascertain how he got up the building , said new york times spokeswoman catherine mathis . \n",
      "Predicted translation: 그는 영국 런던의 다른 개 주에서 주가된 기자들을 대상으로 한 뉴욕 타임즈 지가 조사하고 있다 고 말했다 .\n",
      "Input: the first twin , keith , died earlier in the week , he said . \n",
      "Predicted translation: 쌍둥이 쌍둥이로써 , 쌍둥이가 사망했다 .\n",
      "Input: in the eyes of many , that would make her too disgusting to touch . \n",
      "Predicted translation: 많은 사람들이 달간의 체중을 낼 수 있을 것이라고 말했다 .\n",
      "Input: pew s telephone study of , u . s . adults , including , internet users , was conducted april to may and has a margin of sampling error of plus or minus percentage points . \n",
      "Predicted translation: 미국 내 에서는 성인이 전화 , 통화 , 통화 , 통화 , 통화 , 통화 , 건강에 해로운 있었다 .\n",
      "Input: the announcement came after al maliki and the sheikh met sunday , the first day of a two day official visit . \n",
      "Predicted translation: 알 말리키 총리가 알 말리키 총리가 하루 이틀 만에 회담을 가졌다 .\n",
      "Input:  the senators have a busy day ahead of them , as they meet with senior iraqi officials , coalition leadership and officials from the u . s . embassy , embassy spokesman armand cucciniello said . \n",
      "Predicted translation: 한편 미 상원 외교위원회 소속의 알렌 하카스는 이라크 주재 미 대사관을 방문해 이라크 주재 미국 대사관과 동포원을 방문해 이번 폭탄 테러에 대해 협상을 위해 일을 벌였다 고 전했다 .\n",
      "Input: an australian woman often described as the world s oldest blogger has died at the age of after posting a final message about her ailing health but how she sang a happy song , as i do every day . \n",
      "Predicted translation: 세계 에서 가장 오래된 나이가 들였지만 호주 국민들은 이번 에 경의를 표하는 세계적인 압박을 가해야 한다고 밝혔다 .\n",
      "Input:  some time ago i had to deal with an unusual case a fake priest , marrone told the vatican newspaper l osservatore romano . \n",
      "Predicted translation: 언론은 누군가가 불을 조작하는 것에 대해 해명을 해온 것 같다고 말했다 .\n",
      "Input: the swiss , ranked th by the fifa , are regarded as the weakest among european countries . \n",
      "Predicted translation: 스위스 마터호른에서의 트레킹 스위스 마터호른에서의 트레킹\n",
      "Input: palestinian leader mahmoud abbas and hamas exiled political chief have failed to agree on forming a national unity government during talks in syria . \n",
      "Predicted translation: 마무드 아바스 팔레스타인 자치정부 수반과의 회담은 시리아 정부가 하마스의 통합을 둘러싸고 있는 것이 아니냐는 입장이다 .\n",
      "Input: the agreement came after several days of negotiations and less than a week before a planned visit by u . s . secretary of state condoleezza rice and defense secretary robert gates . \n",
      "Predicted translation: 국방장관은 며칠 뒤 몇 일 동안 라이스 장관과의 회담에 관심이 적었다 .\n",
      "Input: airtran is offering a voluntary early exit program for employees with at least five years of service . \n",
      "Predicted translation: 초원한 층 건물을 차창한 이 프로그램은 년 이내에 처음이다 .\n",
      "Input: seven miners escaped on foot and four others were rescued after saturday s collapse at a mine in shanxi province , the china news agency reported . \n",
      "Predicted translation: 언론은 일 중국 산시성에 있는 납치범 중 명이 구조된 다른 명이 구조된 광부 명이 구조 됐다고 보도했다 .\n",
      "Input: about percent of the teenagers surveyed said they had had sex by age , while percent said they had sex between the ages of and . \n",
      "Predicted translation: 또한 응답자 중 가 성에 대한 성토제의 성 은 성 을 성이라고 답했다 .\n",
      "Input: german chancellor angela merkel has also said she will not attend the ceremony , but frank walter steinmeier , germany s foreign minister , said when he announced her decision that it was not meant as a political protest . \n",
      "Predicted translation: 앙겔라 메르켈 독일 총리는 년 월 독일과의 결혼식이 젤 독일로 이사하지 않을 것이라고 말했다 .\n",
      "Input:  more slightly good news \n",
      "Predicted translation: 좋은 소식을 좋은 소식이 좋은 소식이 좋은 소식이 좋은 소식이 전해지고 있습니다 .\n",
      "Input: the housewife handed the man about , yen , ran outside , and called for help . \n",
      "Predicted translation: 이들은 지난 일 자신이 우울증 치료를 받았으며 일 저녁 시 분께 경찰에 신고했다 .\n",
      "Num of Sample: 100\n",
      "Total Score: 0.02330216624910948\n"
     ]
    }
   ],
   "source": [
    "eval_bleu(transformer, eng_corpus_test[:100], kor_corpus_test[:100], en_tokenizer, ko_tokenizer, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e920a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bleu example\n",
    "# from datasets import load_metric\n",
    "\n",
    "# bleu = load_metric(\"bleu\")\n",
    "# predictions = [[\"I\", \"have\", \"thirty\", \"six\", \"years\"]] \n",
    "# references = [\n",
    "#     [[\"I\", \"am\", \"thirty\", \"six\", \"years\", \"old\"], [\"I\", \"am\", \"thirty\", \"six\"]]\n",
    "# ]\n",
    "# bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "\n",
    "# from datasets import load_metric\n",
    "\n",
    "# bleu = load_metric(\"bleu\")\n",
    "# #bleu = load_metric('sacrebleu')\n",
    "\n",
    "# reference = [kor_corpus_test.split()]\n",
    "# predictions = [translate(eng_corpus_test, transformer, en_tokenizer, ko_tokenizer).split()]\n",
    "\n",
    "# bleu.compute(predictions=predictions, references=reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb75cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "# mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a5d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매 Epoch 마다 제시된 예문에 대한 번역 생성시각화\n",
    "for example in examples:\n",
    "    translate(example, transformer, en_tokenizer, ko_tokenizer, plot_attention=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6af88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728137e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
