/home/seuyon0101/anaconda3/envs/saturi/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  nn.utils.clip_grad_norm_(
Saving model checkpoint to BART-finetuned-eng-to-dial/checkpoint-500
Configuration saved in BART-finetuned-eng-to-dial/checkpoint-500/config.json
Model weights saved in BART-finetuned-eng-to-dial/checkpoint-500/pytorch_model.bin
tokenizer config file saved in BART-finetuned-eng-to-dial/checkpoint-500/tokenizer_config.json
Special tokens file saved in BART-finetuned-eng-to-dial/checkpoint-500/special_tokens_map.json
***** Running Evaluation *****
  Num examples = 101
  Batch size = 2
Training completed. Do not forget to share your model on huggingface.co/models =)
/tmp/ipykernel_32150/3465435333.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  torch.nn.functional.softmax(output.logits)
/tmp/ipykernel_32150/3465435333.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  torch.nn.functional.softmax(output.logits)
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
loading weights file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/pytorch_model.bin from cache at /home/seuyon0101/.cache/huggingface/transformers/37376455ce67919ebfbfd84032875e0989f86be4686adbccec2993d3fafe29c8.0e5b5adc2067434ee15e3a7669c833bd6f2979558cfcb531c8f9e99ad40dae33
Some weights of the model checkpoint at circulus/kobart-trans-en-ko-v2 were not used when initializing BartForConditionalGeneration: ['model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.4.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.2.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layernorm_embedding.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.2.fc2.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.4.fc1.bias', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.1.fc2.bias', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.shared.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.1.fc1.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layernorm_embedding.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'final_logits_bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.5.fc1.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.4.fc1.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'lm_head.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.1.fc1.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.fc1.bias', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layers.4.fc1.weight', 'model.encoder.layers.5.self_attn.k_proj.bias']
- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at circulus/kobart-trans-en-ko-v2 and are newly initialized: ['model.bart.decoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.k_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.1.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.5.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.fc2.weight', 'model.bart.decoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.fc2.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.q_proj.bias', 'model.bart.decoder.layernorm_embedding.weight', 'model.bart.encoder.layers.2.fc1.weight', 'model.bart.decoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.weight', 'model.bart.decoder.layers.4.fc1.bias', 'model.bart.encoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.fc2.weight', 'model.bart.decoder.layers.2.fc2.weight', 'model.bart.decoder.layers.3.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.fc2.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.0.self_attn_layer_norm.weight', 'model.bart.encoder.layers.4.fc1.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.fc2.bias', 'model.bart.decoder.layers.3.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc1.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.final_layer_norm.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.bias', 'model.bart.encoder.layers.5.fc2.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.4.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.1.fc2.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.bias', 'model.bart.encoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.final_layer_norm.bias', 'model.bart.encoder.layers.3.fc2.bias', 'model.bart.decoder.layers.0.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.fc2.bias', 'model.bart.encoder.embed_tokens.weight', 'model.bart.encoder.layers.0.fc1.weight', 'model.bart.encoder.layers.0.fc2.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.k_proj.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.5.fc1.bias', 'model.bart.decoder.layers.5.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.1.self_attn.k_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.weight', 'model.bart.encoder.embed_positions.weight', 'model.bart.encoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.embed_positions.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layernorm_embedding.weight', 'model.bart.decoder.layers.5.self_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc2.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.embed_tokens.weight', 'model.bart.decoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.4.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.self_attn.k_proj.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.final_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.fc1.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc2.weight', 'model.bart.encoder.layers.0.final_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.fc2.bias', 'model.bart.decoder.layers.4.encoder_attn.q_proj.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc2.weight', 'model.lm_head.weight', 'model.bart.decoder.layers.5.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.3.fc1.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.weight', 'model.bart.shared.weight', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.bart.encoder.layernorm_embedding.bias', 'model.bart.decoder.layers.0.final_layer_norm.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.bias', 'model.bart.encoder.layers.4.fc2.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.k_proj.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.2.self_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc1.bias', 'model.bart.decoder.layers.2.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc1.bias', 'model.bart.encoder.layers.0.self_attn.v_proj.weight', 'model.bart.encoder.layers.1.final_layer_norm.weight', 'model.bart.encoder.layers.5.fc1.bias', 'model.bart.encoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layernorm_embedding.bias', 'model.bart.decoder.layers.1.self_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.weight', 'model.bart.encoder.layers.4.fc2.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.fc2.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.bias', 'model.bart.decoder.layers.5.fc1.weight', 'model.bart.encoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.2.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.5.fc1.weight', 'model.bart.encoder.layers.0.fc1.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.weight', 'model.bart.decoder.layers.2.fc1.bias', 'model.bart.encoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.fc1.bias', 'model.bart.encoder.layers.5.final_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.bias', 'model.bart.encoder.layers.3.fc1.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.layers.3.fc2.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.fc2.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn.k_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.3.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.fc2.bias', 'model.bart.encoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.weight', 'model.bart.encoder.layers.2.fc2.bias', 'model.bart.decoder.layers.1.fc2.weight', 'model.bart.encoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.fc1.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.fc1.weight', 'model.bart.decoder.layers.0.fc1.bias', 'model.bart.decoder.layers.2.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.4.fc1.bias', 'model.bart.decoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.weight', 'model.bart.encoder.layers.5.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn.q_proj.weight', 'model.bart.encoder.layers.0.self_attn.v_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using amp fp16 backend
loading weights file https://huggingface.co/circulus/kobart-trans-en-ko-v2/resolve/main/pytorch_model.bin from cache at /home/seuyon0101/.cache/huggingface/transformers/37376455ce67919ebfbfd84032875e0989f86be4686adbccec2993d3fafe29c8.0e5b5adc2067434ee15e3a7669c833bd6f2979558cfcb531c8f9e99ad40dae33
Some weights of the model checkpoint at circulus/kobart-trans-en-ko-v2 were not used when initializing BartForConditionalGeneration: ['model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.4.fc2.bias', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layers.2.fc2.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.decoder.layernorm_embedding.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.1.fc1.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.3.fc2.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.2.fc2.weight', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.2.fc2.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.fc1.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.4.fc1.bias', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layernorm_embedding.weight', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.5.fc2.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.1.fc2.bias', 'model.decoder.layers.5.fc2.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.shared.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.1.fc1.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.fc1.bias', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layernorm_embedding.weight', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'final_logits_bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.decoder.layers.5.fc1.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc2.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.4.fc1.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'lm_head.weight', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc2.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.1.fc1.weight', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.fc1.bias', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layers.4.fc1.weight', 'model.encoder.layers.5.self_attn.k_proj.bias']
- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at circulus/kobart-trans-en-ko-v2 and are newly initialized: ['model.bart.decoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.k_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.1.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.5.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.fc2.weight', 'model.bart.decoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.bias', 'model.bart.decoder.layers.0.fc2.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.q_proj.bias', 'model.bart.decoder.layernorm_embedding.weight', 'model.bart.encoder.layers.2.fc1.weight', 'model.bart.decoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn_layer_norm.weight', 'model.bart.decoder.layers.4.fc1.bias', 'model.bart.encoder.layers.1.fc2.bias', 'model.bart.decoder.layers.4.fc2.weight', 'model.bart.decoder.layers.2.fc2.weight', 'model.bart.decoder.layers.3.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.k_proj.weight', 'model.bart.encoder.layers.0.fc2.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.bias', 'model.bart.encoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.4.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.self_attn.out_proj.weight', 'model.bart.encoder.layers.0.self_attn_layer_norm.weight', 'model.bart.encoder.layers.4.fc1.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.fc2.bias', 'model.bart.decoder.layers.3.fc1.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc1.bias', 'model.bart.decoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.final_layer_norm.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.bias', 'model.bart.decoder.layers.3.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.bias', 'model.bart.encoder.layers.5.fc2.weight', 'model.bart.decoder.layers.0.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.4.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn_layer_norm.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.1.fc2.bias', 'model.bart.encoder.layers.1.self_attn.q_proj.weight', 'model.bart.encoder.layers.4.final_layer_norm.bias', 'model.bart.encoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.final_layer_norm.bias', 'model.bart.encoder.layers.3.fc2.bias', 'model.bart.decoder.layers.0.encoder_attn.q_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.4.fc2.bias', 'model.bart.encoder.embed_tokens.weight', 'model.bart.encoder.layers.0.fc1.weight', 'model.bart.encoder.layers.0.fc2.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.k_proj.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.5.fc1.bias', 'model.bart.decoder.layers.5.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.fc1.weight', 'model.bart.decoder.layers.4.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.1.self_attn.k_proj.bias', 'model.bart.decoder.layers.4.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.q_proj.weight', 'model.bart.encoder.embed_positions.weight', 'model.bart.encoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.embed_positions.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layernorm_embedding.weight', 'model.bart.decoder.layers.5.self_attn.v_proj.weight', 'model.bart.encoder.layers.3.fc2.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.embed_tokens.weight', 'model.bart.decoder.layers.4.final_layer_norm.bias', 'model.bart.decoder.layers.4.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.self_attn.k_proj.weight', 'model.bart.encoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.1.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.final_layer_norm.bias', 'model.bart.encoder.layers.4.self_attn_layer_norm.weight', 'model.bart.decoder.layers.1.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.fc1.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.self_attn.out_proj.bias', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc2.weight', 'model.bart.encoder.layers.0.final_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.weight', 'model.bart.encoder.layers.5.fc2.bias', 'model.bart.decoder.layers.4.encoder_attn.q_proj.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.k_proj.bias', 'model.bart.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn.v_proj.bias', 'model.bart.encoder.layers.5.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.self_attn.q_proj.bias', 'model.bart.encoder.layers.4.self_attn.v_proj.bias', 'model.bart.decoder.layers.0.self_attn.k_proj.weight', 'model.bart.decoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc2.weight', 'model.lm_head.weight', 'model.bart.decoder.layers.5.encoder_attn.v_proj.bias', 'model.bart.decoder.layers.3.fc1.weight', 'model.bart.encoder.layers.2.self_attn.k_proj.weight', 'model.bart.shared.weight', 'model.bart.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.bart.encoder.layernorm_embedding.bias', 'model.bart.decoder.layers.0.final_layer_norm.bias', 'model.bart.decoder.layers.2.self_attn.k_proj.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.bias', 'model.bart.encoder.layers.4.fc2.weight', 'model.bart.decoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.1.self_attn.k_proj.bias', 'model.bart.encoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.4.self_attn.q_proj.weight', 'model.bart.encoder.layers.2.final_layer_norm.bias', 'model.bart.encoder.layers.5.self_attn.k_proj.weight', 'model.bart.decoder.layers.2.encoder_attn.out_proj.bias', 'model.bart.encoder.layers.2.self_attn_layer_norm.weight', 'model.bart.encoder.layers.2.fc1.bias', 'model.bart.decoder.layers.2.self_attn.q_proj.bias', 'model.bart.decoder.layers.3.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn_layer_norm.weight', 'model.bart.decoder.layers.3.self_attn_layer_norm.weight', 'model.bart.encoder.layers.1.fc1.bias', 'model.bart.encoder.layers.0.self_attn.v_proj.weight', 'model.bart.encoder.layers.1.final_layer_norm.weight', 'model.bart.encoder.layers.5.fc1.bias', 'model.bart.encoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn_layer_norm.bias', 'model.bart.encoder.layers.3.self_attn_layer_norm.bias', 'model.bart.decoder.layernorm_embedding.bias', 'model.bart.decoder.layers.1.self_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.weight', 'model.bart.encoder.layers.4.fc2.bias', 'model.bart.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.bart.decoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.5.fc2.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.3.final_layer_norm.weight', 'model.bart.decoder.layers.2.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.q_proj.bias', 'model.bart.decoder.layers.5.fc1.weight', 'model.bart.encoder.layers.5.self_attn_layer_norm.bias', 'model.bart.decoder.layers.2.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.5.fc1.weight', 'model.bart.encoder.layers.0.fc1.bias', 'model.bart.encoder.layers.3.self_attn.v_proj.weight', 'model.bart.decoder.layers.2.fc1.bias', 'model.bart.encoder.layers.1.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.1.fc1.bias', 'model.bart.encoder.layers.5.final_layer_norm.bias', 'model.bart.decoder.layers.0.self_attn.q_proj.bias', 'model.bart.decoder.layers.0.self_attn.out_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.bias', 'model.bart.encoder.layers.3.fc1.weight', 'model.bart.decoder.layers.3.encoder_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.layers.3.fc2.bias', 'model.bart.decoder.layers.5.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.2.self_attn.v_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.out_proj.weight', 'model.bart.decoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn_layer_norm.bias', 'model.bart.decoder.layers.5.fc2.weight', 'model.bart.encoder.layers.0.self_attn.out_proj.bias', 'model.bart.decoder.layers.4.self_attn.k_proj.bias', 'model.bart.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.bart.decoder.layers.3.encoder_attn.v_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.k_proj.weight', 'model.bart.decoder.layers.2.self_attn.q_proj.weight', 'model.bart.decoder.layers.0.fc2.bias', 'model.bart.encoder.layers.5.final_layer_norm.weight', 'model.bart.decoder.layers.0.self_attn.v_proj.bias', 'model.bart.decoder.layers.5.self_attn.k_proj.weight', 'model.bart.encoder.layers.2.fc2.bias', 'model.bart.decoder.layers.1.fc2.weight', 'model.bart.encoder.layers.3.self_attn.out_proj.bias', 'model.bart.encoder.layers.2.final_layer_norm.weight', 'model.bart.decoder.layers.0.encoder_attn.q_proj.weight', 'model.bart.decoder.layers.5.encoder_attn.out_proj.bias', 'model.bart.decoder.layers.5.encoder_attn.q_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.weight', 'model.bart.encoder.layers.2.self_attn.q_proj.bias', 'model.bart.encoder.layers.5.self_attn.v_proj.bias', 'model.bart.encoder.layers.0.self_attn.k_proj.weight', 'model.bart.encoder.layers.0.self_attn.q_proj.weight', 'model.bart.decoder.layers.1.self_attn.v_proj.weight', 'model.bart.decoder.layers.4.fc1.weight', 'model.bart.encoder.layers.2.self_attn.out_proj.bias', 'model.bart.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.bart.encoder.layers.1.fc1.weight', 'model.bart.decoder.layers.0.fc1.bias', 'model.bart.decoder.layers.2.encoder_attn.out_proj.weight', 'model.bart.decoder.layers.0.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.4.fc1.bias', 'model.bart.decoder.layers.2.self_attn.v_proj.bias', 'model.bart.decoder.layers.2.encoder_attn.v_proj.weight', 'model.bart.encoder.layers.1.self_attn.v_proj.bias', 'model.bart.encoder.layers.1.self_attn.out_proj.bias', 'model.bart.encoder.layers.3.final_layer_norm.bias', 'model.bart.decoder.layers.1.encoder_attn.v_proj.bias', 'model.bart.encoder.layers.3.self_attn.k_proj.bias', 'model.bart.encoder.layers.4.self_attn.k_proj.weight', 'model.bart.encoder.layers.5.self_attn.q_proj.weight', 'model.bart.encoder.layers.3.self_attn.q_proj.weight', 'model.bart.encoder.layers.0.self_attn.v_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
***** Running training *****
  Num examples = 1001
  Num Epochs = 1
  Instantaneous batch size per device = 2
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 16
  Total optimization steps = 31
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
***** Running Evaluation *****
  Num examples = 101
  Batch size = 2
Training completed. Do not forget to share your model on huggingface.co/models =)